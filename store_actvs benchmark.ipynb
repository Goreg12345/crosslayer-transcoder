{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a2f0134f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/metrics/g/crosslayer-transcoder/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA available: False\n",
      "No CUDA devices found\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import nnsight\n",
    "import datasets\n",
    "import activation_server.text_dataset as text_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import h5py\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
    "\n",
    "\n",
    "# Check if CUDA is available\n",
    "cuda_available = torch.cuda.is_available()\n",
    "print(\"CUDA available:\", cuda_available)\n",
    "\n",
    "if cuda_available:\n",
    "    # Number of GPUs\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(\"Number of GPUs:\", num_gpus)\n",
    "\n",
    "    # List each deviceâ€™s name\n",
    "    for i in range(num_gpus):\n",
    "        name = torch.cuda.get_device_name(i)\n",
    "        print(f\"GPU {i}: {name}\")\n",
    "else:\n",
    "    print(\"No CUDA devices found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4011e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nnsight.LanguageModel('openai-community/gpt2', device_map='auto', dispatch=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83ecfd48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       "  (generator): Generator(\n",
       "    (streamer): Streamer()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc47ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = datasets.load_dataset('Skylion007/openwebtext', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b6a7a615",
   "metadata": {},
   "outputs": [],
   "source": [
    "token_dataset = text_dataset.TextDataset(\n",
    "    dataset,\n",
    "    model.tokenizer,\n",
    "    40,\n",
    "    drop_last_batch=False,\n",
    "    seq_len=1023,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7de8e1fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "text_dataset_loader = iter(\n",
    "    DataLoader(\n",
    "        token_dataset,\n",
    "        batch_size=None,\n",
    "        shuffle=False,\n",
    "        num_workers=5,\n",
    "        prefetch_factor=5,\n",
    "        worker_init_fn=text_dataset.worker_init_fn,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfe7f5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd13223f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "440it [00:10, 43.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Throughput:\n",
      "Samples/sec: 1794505.3\n",
      "Batches/sec: 43.9\n",
      "Average batch size: 40920.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Run for 10 seconds to get average throughput\n",
    "start_time = time.time()\n",
    "end_time = start_time + 1\n",
    "total_samples = 0\n",
    "total_batches = 0\n",
    "\n",
    "for batch in tqdm(text_dataset_loader):\n",
    "    if time.time() > end_time:\n",
    "        break\n",
    "    total_samples += batch.numel()\n",
    "    total_batches += 1\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "samples_per_sec = total_samples / elapsed\n",
    "batches_per_sec = total_batches / elapsed\n",
    "\n",
    "print(f\"\\nThroughput:\")\n",
    "print(f\"Samples/sec: {samples_per_sec:.1f}\")\n",
    "print(f\"Batches/sec: {batches_per_sec:.1f}\")\n",
    "print(f\"Average batch size: {total_samples/total_batches:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "610885c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_activations(model, tokens):\n",
    "    with model.trace(tokens) as tracer:\n",
    "        mlp_ins = []\n",
    "        mlp_outs = []\n",
    "        for i in range(12):\n",
    "            mlp_in = model.transformer.h[i].ln_2.input.save()\n",
    "            mlp_ins.append(mlp_in)\n",
    "            mlp_out = model.transformer.h[i].mlp.output.save()\n",
    "            mlp_outs.append(mlp_out)\n",
    "    # batch layer in/out d_model\n",
    "    mlp_ins = torch.stack(mlp_ins, dim=2)\n",
    "    mlp_outs = torch.stack(mlp_outs, dim=2)\n",
    "    mlp_acts = torch.stack([mlp_ins, mlp_outs], dim=2)\n",
    "    return mlp_acts  # batch seq_len in/out n_layer d_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27c3be0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running activation extraction benchmark...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "4it [00:57, 14.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Activation Extraction Throughput:\n",
      "Tokens/sec: 2,839.1\n",
      "Batches/sec: 0.1\n",
      "Average batch size: 40920.0 tokens\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Run benchmark for 10 seconds to measure activation extraction throughput\n",
    "start_time = time.time()\n",
    "end_time = start_time + 50\n",
    "total_tokens = 0\n",
    "total_batches = 0\n",
    "\n",
    "print(\"Running activation extraction benchmark...\")\n",
    "for batch in tqdm(text_dataset_loader):\n",
    "    if time.time() > end_time:\n",
    "        break\n",
    "        \n",
    "    # prepend BOS token like in main loop\n",
    "    batch = torch.roll(batch, shifts=1, dims=1)\n",
    "    batch[:, 0] = model.config.bos_token_id\n",
    "    \n",
    "    # Extract activations\n",
    "    mlp_acts = extract_activations(model, batch)\n",
    "    \n",
    "    total_tokens += batch.numel()\n",
    "    total_batches += 1\n",
    "\n",
    "elapsed = time.time() - start_time\n",
    "tokens_per_sec = total_tokens / elapsed\n",
    "batches_per_sec = total_batches / elapsed\n",
    "\n",
    "print(f\"\\nActivation Extraction Throughput:\")\n",
    "print(f\"Tokens/sec: {tokens_per_sec:,.1f}\")\n",
    "print(f\"Batches/sec: {batches_per_sec:.1f}\")\n",
    "print(f\"Average batch size: {total_tokens/total_batches:.1f} tokens\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2d4ea7",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "with model.trace(batch):\n",
    "    print('processing')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d2e429",
   "metadata": {},
   "outputs": [],
   "source": [
    "alskdj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84ffc2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % done\n",
      "0.4092 % done\n",
      "0.8184 % done\n",
      "1.2276 % done\n",
      "1.6368 % done\n",
      "2.046 % done\n",
      "2.4552 % done\n",
      "2.8644 % done\n",
      "3.2736 % done\n",
      "3.6828 % done\n",
      "4.092 % done\n",
      "4.501200000000001 % done\n",
      "4.9104 % done\n",
      "5.3196 % done\n",
      "5.7288 % done\n",
      "6.138 % done\n",
      "6.5472 % done\n",
      "6.9564 % done\n",
      "7.3656 % done\n",
      "7.7748 % done\n",
      "8.184 % done\n",
      "8.5932 % done\n",
      "9.002400000000002 % done\n",
      "9.4116 % done\n",
      "9.8208 % done\n",
      "10.23 % done\n",
      "10.6392 % done\n",
      "11.048399999999999 % done\n",
      "11.4576 % done\n",
      "11.8668 % done\n",
      "12.276 % done\n",
      "12.6852 % done\n",
      "13.0944 % done\n",
      "13.503599999999999 % done\n",
      "13.9128 % done\n",
      "14.322 % done\n",
      "14.7312 % done\n",
      "15.140400000000001 % done\n",
      "15.5496 % done\n",
      "15.9588 % done\n",
      "16.368 % done\n",
      "16.7772 % done\n",
      "17.1864 % done\n",
      "17.5956 % done\n",
      "18.004800000000003 % done\n",
      "18.414 % done\n",
      "18.8232 % done\n",
      "19.2324 % done\n",
      "19.6416 % done\n",
      "20.0508 % done\n",
      "20.46 % done\n",
      "20.8692 % done\n",
      "21.2784 % done\n",
      "21.6876 % done\n",
      "22.096799999999998 % done\n",
      "22.506 % done\n",
      "22.9152 % done\n",
      "23.3244 % done\n",
      "23.7336 % done\n",
      "24.1428 % done\n",
      "24.552 % done\n",
      "24.9612 % done\n",
      "25.3704 % done\n",
      "25.779600000000002 % done\n",
      "26.1888 % done\n",
      "26.598 % done\n",
      "27.007199999999997 % done\n",
      "27.416400000000003 % done\n",
      "27.8256 % done\n",
      "28.2348 % done\n",
      "28.644 % done\n",
      "29.0532 % done\n",
      "29.4624 % done\n",
      "29.871599999999997 % done\n",
      "30.280800000000003 % done\n",
      "30.69 % done\n",
      "31.0992 % done\n",
      "31.508399999999998 % done\n",
      "31.9176 % done\n",
      "32.3268 % done\n",
      "32.736 % done\n",
      "33.1452 % done\n",
      "33.5544 % done\n",
      "33.9636 % done\n",
      "34.3728 % done\n",
      "34.782000000000004 % done\n",
      "35.1912 % done\n",
      "35.6004 % done\n",
      "36.009600000000006 % done\n",
      "36.418800000000005 % done\n",
      "36.828 % done\n",
      "37.2372 % done\n",
      "37.6464 % done\n",
      "38.0556 % done\n",
      "38.4648 % done\n",
      "38.873999999999995 % done\n",
      "39.2832 % done\n",
      "39.6924 % done\n",
      "40.1016 % done\n",
      "40.5108 % done\n",
      "40.92 % done\n",
      "41.3292 % done\n",
      "41.7384 % done\n",
      "42.147600000000004 % done\n",
      "42.5568 % done\n",
      "42.966 % done\n",
      "43.3752 % done\n",
      "43.7844 % done\n",
      "44.193599999999996 % done\n",
      "44.602799999999995 % done\n",
      "45.012 % done\n",
      "45.4212 % done\n",
      "45.8304 % done\n",
      "46.239599999999996 % done\n",
      "46.6488 % done\n",
      "47.058 % done\n",
      "47.4672 % done\n",
      "47.876400000000004 % done\n",
      "48.2856 % done\n",
      "48.6948 % done\n",
      "49.104 % done\n",
      "49.513200000000005 % done\n",
      "49.9224 % done\n",
      "50.3316 % done\n",
      "50.7408 % done\n",
      "51.15 % done\n",
      "51.559200000000004 % done\n",
      "51.9684 % done\n",
      "52.3776 % done\n",
      "52.7868 % done\n",
      "53.196 % done\n",
      "53.605199999999996 % done\n",
      "54.014399999999995 % done\n",
      "54.42360000000001 % done\n",
      "54.832800000000006 % done\n",
      "55.242000000000004 % done\n",
      "55.6512 % done\n",
      "56.0604 % done\n",
      "56.4696 % done\n",
      "56.8788 % done\n",
      "57.288 % done\n",
      "57.6972 % done\n",
      "58.1064 % done\n",
      "58.5156 % done\n",
      "58.9248 % done\n",
      "59.333999999999996 % done\n",
      "59.743199999999995 % done\n",
      "60.15239999999999 % done\n",
      "60.561600000000006 % done\n",
      "60.970800000000004 % done\n",
      "61.38 % done\n",
      "61.7892 % done\n",
      "62.1984 % done\n",
      "62.6076 % done\n",
      "63.016799999999996 % done\n",
      "63.426 % done\n",
      "63.8352 % done\n",
      "64.2444 % done\n",
      "64.6536 % done\n",
      "65.0628 % done\n",
      "65.472 % done\n",
      "65.88119999999999 % done\n",
      "66.2904 % done\n",
      "66.6996 % done\n",
      "67.1088 % done\n",
      "67.518 % done\n",
      "67.9272 % done\n",
      "68.3364 % done\n",
      "68.7456 % done\n",
      "69.15480000000001 % done\n",
      "69.56400000000001 % done\n",
      "69.9732 % done\n",
      "70.3824 % done\n",
      "70.7916 % done\n",
      "71.2008 % done\n",
      "71.61 % done\n",
      "72.01920000000001 % done\n",
      "72.42840000000001 % done\n",
      "72.83760000000001 % done\n",
      "73.24680000000001 % done\n",
      "73.656 % done\n",
      "74.0652 % done\n",
      "74.4744 % done\n",
      "74.8836 % done\n",
      "75.2928 % done\n",
      "75.702 % done\n",
      "76.1112 % done\n",
      "76.5204 % done\n",
      "76.9296 % done\n",
      "77.33879999999999 % done\n",
      "77.74799999999999 % done\n",
      "78.1572 % done\n",
      "78.5664 % done\n",
      "78.9756 % done\n",
      "79.3848 % done\n",
      "79.794 % done\n",
      "80.2032 % done\n",
      "80.6124 % done\n",
      "81.0216 % done\n",
      "81.4308 % done\n",
      "81.84 % done\n",
      "82.2492 % done\n",
      "82.6584 % done\n",
      "83.0676 % done\n",
      "83.4768 % done\n",
      "83.88600000000001 % done\n",
      "84.29520000000001 % done\n",
      "84.7044 % done\n",
      "85.1136 % done\n",
      "85.5228 % done\n",
      "85.932 % done\n",
      "86.3412 % done\n",
      "86.7504 % done\n",
      "87.1596 % done\n",
      "87.5688 % done\n",
      "87.978 % done\n",
      "88.38719999999999 % done\n",
      "88.79639999999999 % done\n",
      "89.20559999999999 % done\n",
      "89.61479999999999 % done\n",
      "90.024 % done\n",
      "90.4332 % done\n",
      "90.8424 % done\n",
      "91.2516 % done\n",
      "91.6608 % done\n",
      "92.07 % done\n",
      "92.47919999999999 % done\n",
      "92.8884 % done\n",
      "93.2976 % done\n",
      "93.7068 % done\n",
      "94.116 % done\n",
      "94.5252 % done\n",
      "94.9344 % done\n",
      "95.3436 % done\n",
      "95.75280000000001 % done\n",
      "96.162 % done\n",
      "96.5712 % done\n",
      "96.9804 % done\n",
      "97.3896 % done\n",
      "97.7988 % done\n",
      "98.208 % done\n",
      "98.61720000000001 % done\n",
      "99.02640000000001 % done\n",
      "99.43560000000001 % done\n",
      "99.8448 % done\n"
     ]
    }
   ],
   "source": [
    "store_path = '/var/local/glang/activations'\n",
    "filename = 'clt-activations-10M.h5'\n",
    "store_size = 10000000\n",
    "actv_size = model.config.n_embd\n",
    "\n",
    "with h5py.File(os.path.join(store_path, filename), \"w\") as f:\n",
    "    h5_dataset = f.create_dataset(\n",
    "        'tensor', (store_size, 2, model.config.n_layer, model.config.n_embd), dtype='float32'\n",
    "    )\n",
    "\n",
    "    h5_pointer = 0\n",
    "    for batch in text_dataset_loader:\n",
    "        print(h5_pointer / store_size * 100, \"% done\")\n",
    "        # prepend BOS (important)\n",
    "        batch = torch.roll(batch, shifts=1, dims=1)\n",
    "        batch[:, 0] = model.config.bos_token_id\n",
    "\n",
    "        # extract activations\n",
    "        mlp_acts = extract_activations(model, batch)\n",
    "\n",
    "        # store activations\n",
    "        mlp_acts = mlp_acts.flatten(0, 1)\n",
    "        n_acts = mlp_acts.shape[0]\n",
    "\n",
    "        if h5_pointer + n_acts > store_size:\n",
    "            h5_dataset[h5_pointer:] = (\n",
    "                mlp_acts[: int(store_size - h5_pointer)].cpu().numpy()\n",
    "            )\n",
    "            break\n",
    "        else:\n",
    "            h5_dataset[h5_pointer : h5_pointer + n_acts] = mlp_acts.cpu().numpy()\n",
    "            h5_pointer += n_acts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ddf89d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
