{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# PyTorch DTensor API: distribute_tensor() and distribute_module()\n",
        "\n",
        "PyTorch's **DTensor (Distributed Tensor)** API provides JAX-like explicit tensor sharding capabilities. The key functions `distribute_tensor()` and `distribute_module()` give you fine-grained control over how tensors and modules are distributed across devices.\n",
        "\n",
        "## Key Concepts\n",
        "\n",
        "**DTensor** represents a tensor that is distributed across multiple devices with explicit sharding specifications, similar to JAX's sharded arrays.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "from torch.distributed.tensor import (\n",
        "    DeviceMesh,\n",
        "    DTensor,\n",
        "    Replicate,\n",
        "    Shard,\n",
        "    distribute_tensor,\n",
        "    distribute_module,\n",
        ")\n",
        "from torch.distributed.tensor.placement_types import Placement\n",
        "import os\n",
        "\n",
        "# Setup for demonstration\n",
        "print(\"PyTorch DTensor API Demo\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "# For this demo, we'll use CPU to simulate multiple devices\n",
        "WORLD_SIZE = 4  # Simulate 4 devices\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## 1. torch.distribute_tensor()\n",
        "\n",
        "`distribute_tensor()` converts a regular PyTorch tensor into a **DTensor** (Distributed Tensor) with explicit sharding across devices.\n",
        "\n",
        "**Function Signature:**\n",
        "```python\n",
        "distribute_tensor(\n",
        "    tensor: torch.Tensor,\n",
        "    device_mesh: DeviceMesh,\n",
        "    placements: List[Placement],\n",
        "    src_data_rank: int = 0\n",
        ") -> DTensor\n",
        "```\n",
        "\n",
        "**Parameters:**\n",
        "- `tensor`: The tensor to distribute\n",
        "- `device_mesh`: Defines the device topology (similar to JAX's Mesh)\n",
        "- `placements`: How to shard/replicate each dimension (similar to JAX's PartitionSpec)\n",
        "- `src_data_rank`: Which rank holds the original data\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
