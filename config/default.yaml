# Default configuration for CrossLayer Transcoder training
# This file uses Lightning CLI's automatic class construction

seed_everything: 42

trainer:
  # max_steps is number of gradient updates, not number of batches
  max_steps: 2_500  # 20M tokens
  #limit_train_batches: 10_000
  val_check_interval: 200
  limit_val_batches: 1
  check_val_every_n_epoch: null
  enable_checkpointing: false  # We use custom end-of-training checkpoint
  num_sanity_val_steps: 0  # Can't run replacement model before standardizers are initialized
  precision: "16-mixed"
  accelerator: "gpu"
  devices: 1  # 4; [1] for cuda:1
  accumulate_grad_batches: 2
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "crosslayer-transcoder"
      save_dir: "./wandb"
  callbacks:
    - class_path: utils.callbacks.EndOfTrainingCheckpointCallback
      init_args:
        checkpoint_dir: "checkpoints"
    #- class_path: utils.callbacks.TensorBoardProfilerCallback
    #  init_args:
    #    log_dir: "log/profiler"

model:
  class_path: model.clt_lightning.CrossLayerTranscoderModule
  init_args:
    # Model architecture parameters
    d_acts: 768
    d_features: 6144  # 768 * 8
    n_layers: 12
    
    # Nonlinearity parameters
    nonlinearity_theta: 0.03
    nonlinearity_bandwidth: 1.0
    
    # Standardizer parameters
    activation_dim: 768
    
    # Loss hyperparameters
    lambda_sparsity: 0.0004
    c_sparsity: 0.1
    
    # Optimization
    learning_rate: 1e-3
    
    # Compilation
    compile: true

    # Metrics
    replacement_model_name: "openai-community/gpt2"
    replacement_model_device_map: "cuda:0"
    replacement_model_loader_batch_size: 5

    # Dead features
    compute_dead_features: true
    compute_dead_features_every: 1000

data:
  class_path: data.datamodule.ActivationDataModule
  init_args:
    # Buffer settings
    buffer_size: 2_000_000
    n_in_out: 2
    n_layers: 12
    activation_dim: 768
    dtype: "float16"
    max_batch_size: 50000
    
    # Model settings for activation generation
    model_name: "openai-community/gpt2"
    model_dtype: "float32"
    
    # Dataset settings
    dataset_name: "Skylion007/openwebtext"
    dataset_split: "train"
    max_sequence_length: 1024
    
    # Generation settings
    generation_batch_size: 2
    refresh_interval: 0.1
    
    # Memory settings
    shared_memory_name: "activation_buffer"
    timeout_seconds: 30
    
    # File paths
    init_file: "/var/local/glang/activations/clt-activations-10M-shuffled_fp16.h5"
    
    # DataLoader settings
    batch_size: 4000
    num_workers: 20
    prefetch_factor: 2
    shuffle: true
    persistent_workers: true
    pin_memory: true
    
    use_shared_memory: true

    # Device configuration
    device_map: "cuda:2"  # "cpu", "auto", "cuda:0", "cuda:0,1,2,3"

    # WandB logging configuration for data generation
    wandb_logging:
      enabled: true                           # Enable WandB logging for data generation
      project: "crosslayer-transcoder"        # WandB project (should match trainer logger)
      group: null                             # Group name (null = auto-generated from training run)
      run_name: "data-generator"              # Run name suffix
      tags: ["data-generation"]               # Tags for the data generation run
      save_dir: "./wandb"                     # Directory for WandB files
      log_interval: 5.0                       # Logging interval in seconds

ckpt_path: null