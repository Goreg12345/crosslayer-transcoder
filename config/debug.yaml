# Debug configuration for CrossLayer Transcoder training
# This file uses Lightning CLI's automatic class construction

seed_everything: 42

trainer:
  max_steps: 20
  val_check_interval: 2
  limit_val_batches: 1
  enable_checkpointing: false  # We use custom end-of-training checkpoint
  num_sanity_val_steps: 0  # Can't run replacement model before standardizers are initialized
  precision: "16-mixed"
  accelerator: "gpu"
  devices: [0]  # [0] means cuda:0
  accumulate_grad_batches: 1
  logger:  # WandB logger is recommended but other loggers are supported as well
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: "clt"
      name: "debug-jumprelu"
      save_dir: "./wandb"
  callbacks:
    - class_path: crosslayer_transcoder.utils.callbacks.EndOfTrainingCheckpointCallback
      init_args:
        checkpoint_dir: "checkpoints_debug"

model:
  class_path: crosslayer_transcoder.model.clt_lightning.JumpReLUCrossLayerTranscoderModule
  init_args:
    model:
      class_path: crosslayer_transcoder.model.clt.CrossLayerTranscoder
      init_args:
        encoder:
          class_path: crosslayer_transcoder.model.clt.Encoder
          init_args:
            d_acts: 16
            d_features: 48
            n_layers: 2

        decoder:
          class_path: crosslayer_transcoder.model.clt.CrosslayerDecoder
          init_args:
            d_acts: 16
            d_features: 48
            n_layers: 2

        nonlinearity:
          class_path: crosslayer_transcoder.model.jumprelu.JumpReLU
          init_args:
            theta: 0.03
            bandwidth: 0.01
            n_layers: 2
            d_features: 48

        input_standardizer:
          class_path: crosslayer_transcoder.model.standardize.DimensionwiseInputStandardizer
          init_args:
            n_layers: 2
            activation_dim: 16

        output_standardizer:
          class_path: crosslayer_transcoder.model.standardize.DimensionwiseOutputStandardizer
          init_args:
            n_layers: 2
            activation_dim: 16

    # Pre-constructed replacement model
    replacement_model:
      class_path: crosslayer_transcoder.metrics.replacement_model_accuracy.ReplacementModelAccuracy
      init_args:
        model_name: "openai-community/gpt2"
        device_map: "cuda:0"  # should match trainer.devices
        loader_batch_size: 2
    
    # Pre-constructed dead features metric
    dead_features:
      class_path: crosslayer_transcoder.metrics.dead_features.DeadFeatures
      init_args:
        n_features: 48
        n_layers: 2
        return_per_layer: true
        return_log_freqs: true
        return_neuron_indices: true
    

    # Training parameters
    learning_rate: 1e-4
    compile: true  # if using torch.compile
    lr_decay_step: 80_000  # lr is scaled by lr_decay_factor after this many steps
    lr_decay_factor: 0.1

    compute_dead_features: true
    compute_dead_features_every: 2

data:
  class_path: crosslayer_transcoder.data.datamodule.ActivationDataModule
  init_args:
    buffer_size: 200
    n_in_out: 2
    n_layers: 2
    activation_dim: 16
    dtype: "float16"
    max_batch_size: 50

    model_name: "openai-community/gpt2"
    model_dtype: "float32"
    
    # Dataset settings
    dataset_name: "Skylion007/openwebtext"
    dataset_split: "train"
    max_sequence_length: 128

    generation_batch_size: 2
    refresh_interval: 0.05

    shared_memory_name: "activation_buffer_debug"
    timeout_seconds: 10

    init_file: null

    batch_size: 8
    num_workers: 0
    prefetch_factor: 2
    shuffle: true
    persistent_workers: false
    pin_memory: false

    minimum_fill_threshold: 0.1
    use_shared_memory: false

    use_shared_memory: true

    # Device configuration
    device_map: "cuda:0"  # "cpu", "auto", "cuda:0", "cuda:0,1,2,3"
    deployment_policy: "gpu_only"  # "cpu_only", "gpu_only", or "dynamic"
    # dynamic will use CPU and only GPU if the buffer is almost empty to refill fast. Use this if you use a single GPU and have a beefy CPU.

    # WandB logging configuration for data generation
    wandb_logging:
      enabled: false
      project: "clt"
      group: null
      run_name: "data-generator-debug-jumprelu"
      tags: ["data-generation", "debug"]
      save_dir: "./wandb"
      log_interval: 5.0

ckpt_path: null