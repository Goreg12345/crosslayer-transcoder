{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd269ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a20f4b",
   "metadata": {},
   "source": [
    "# Overview of what we want to achieve:\n",
    "\n",
    "* develop replacement model\n",
    "* develop way to sample from the replacement model\n",
    "* need to make a dataset and dataloader\n",
    "* write the evaluation + calculate the accuracy\n",
    "* need to put this as a Validation Metric to track in our training loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bf9705",
   "metadata": {},
   "source": [
    "## What are the different implementation options?\n",
    "\n",
    "1. Combine gpt2 + CLT; use hooks\n",
    "* we load gpt2 in memory\n",
    "* we load (or have) the CLT in memory\n",
    "* we use nnsight to put a lot of hooks into gpt2\n",
    "* each hook runs part of the clt\n",
    "\n",
    "2. write a replacement model class from scratch\n",
    "* basically like a transformer but replace the MLPs with they weights from the clt\n",
    "* read in attention weights, W_enc, W_pos, W_unemb from gpt2\n",
    "* read in CLT weights from the CLT\n",
    "\n",
    "### Things to consider for decision:\n",
    "* easy to read and understand\n",
    "* whether it's okay to ie during validation offload the CLT and gpt2 to load this model\n",
    "\n",
    "\n",
    "### local replacement model\n",
    "* run gpt2 on some prompts, store mlp_in, mlp_out\n",
    "* run through the CLT model using activations from gpt2\n",
    "* calculate the error terms recons - mlp_out\n",
    "\n",
    "### What is the replacement model used for?\n",
    "* just for evaluation\n",
    "\n",
    "## Conclusion:\n",
    "\n",
    "Since the replacement model will only be used as a validation metric and has no use cases downstream, the best option to implement this as a hook function model with nnsight because we won't need to load in weights, we can simply use the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c3a9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('hi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89892f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class ReplacementModel(torch.nn.Module):\n",
    "    def __init__(self, gpt2, clt):\n",
    "        super(ReplacementModel, self).__init__()\n",
    "        self.gpt2 = gpt2\n",
    "        self.clt = clt\n",
    "\n",
    "    def overwrite_mlp_out(self, tokens):\n",
    "        with self.gpt2.trace(tokens) as tracer:\n",
    "            mlp_ins = []\n",
    "            mlp_outs = []\n",
    "            for i in range(12):\n",
    "                mlp_in = self.gpt2.transformer.h[i].ln_2.input.save()\n",
    "                mlp_ins.append(mlp_in)\n",
    "                mlp_out = self.gpt2.transformer.h[i].mlp.output.save()\n",
    "                self.gpt2.transformer.h[i].mlp.output = torch.zeros_like(mlp_out)\n",
    "                mlp_outs.append(mlp_out)\n",
    "        # batch layer in/out d_model\n",
    "        mlp_ins = torch.stack(mlp_ins, dim=2)\n",
    "        mlp_outs = torch.stack(mlp_outs, dim=2)\n",
    "        mlp_acts = torch.stack([mlp_ins, mlp_outs], dim=2)\n",
    "        return mlp_acts  # batch seq_len in/out n_layer d_model\n",
    "    \n",
    "    def forward(self, tokens):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8caa778",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'crosslayer-transcoder-5Jm3OZ38-py3.12 (Python 3.12.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import nnsight\n",
    "\n",
    "gpt2 = nnsight.LanguageModel('openai-community/gpt2', device_map='auto', dispatch=True)\n",
    "gpt2.require_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108da02",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'crosslayer-transcoder-5Jm3OZ38-py3.12 (Python 3.12.9)' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "dataset = datasets.load_dataset('Skylion007/openwebtext', split='train')\n",
    "token_dataset = text_dataset.TextDataset(\n",
    "    dataset,\n",
    "    model.tokenizer,\n",
    "    40,\n",
    "    drop_last_batch=False,\n",
    "    seq_len=1023,\n",
    ")\n",
    "text_dataset_loader = iter(\n",
    "    DataLoader(\n",
    "        token_dataset,\n",
    "        batch_size=None,\n",
    "        shuffle=False,\n",
    "        num_workers=5,\n",
    "        prefetch_factor=5,\n",
    "        worker_init_fn=text_dataset.worker_init_fn,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "740e5f79",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in text_dataset_loader:\n",
    "\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "crosslayer-transcoder-5Jm3OZ38-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
