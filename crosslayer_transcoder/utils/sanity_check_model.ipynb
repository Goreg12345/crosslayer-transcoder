{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e36e2f4",
   "metadata": {},
   "source": [
    "# Model Sanity checks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cecc0fd",
   "metadata": {},
   "source": [
    "## Load into `crosslayer-transcoder` arch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7531d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b802f1a7433c4a82b12ae6355db951ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JumpReLUCrossLayerTranscoderModule(\n",
      "  (model): CrossLayerTranscoder(\n",
      "    (encoder): Encoder()\n",
      "    (decoder): CrosslayerDecoder()\n",
      "    (nonlinearity): JumpReLU()\n",
      "    (input_standardizer): DimensionwiseInputStandardizer()\n",
      "    (output_standardizer): DimensionwiseOutputStandardizer()\n",
      "  )\n",
      "  (replacement_model): ReplacementModelAccuracy(\n",
      "    (replacement_model): ReplacementModel()\n",
      "  )\n",
      "  (dead_features): DeadFeatures()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from crosslayer_transcoder.utils.module_builder import build_module_from_config, yaml_to_config\n",
    "from crosslayer_transcoder.utils.checkpoints import load_model_from_lightning_checkpoint\n",
    "\n",
    "config_path = \"../../config/circuit-tracer.yaml\"\n",
    "checkpoint_path = \"../checkpoints/clt.ckpt\"\n",
    "\n",
    "config = yaml_to_config(config_path)\n",
    "clt_module = build_module_from_config(config)\n",
    "\n",
    "clt_module = load_model_from_lightning_checkpoint(clt_module, checkpoint_path)\n",
    "\n",
    "print(clt_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5030984",
   "metadata": {},
   "source": [
    "### Collect Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597a9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "prompt = (\n",
    "    \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    ")\n",
    "llm = LanguageModel(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cda0fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "mlp_in_activations = []\n",
    "\n",
    "with llm.trace(prompt) as trace:\n",
    "    for layer in range(12):\n",
    "        layer_activations = llm.transformer.h[layer].ln_2.input.save()\n",
    "        mlp_in_activations.append(layer_activations.squeeze(0))\n",
    "\n",
    "mlp_in_activations = torch.stack(mlp_in_activations, dim=0)\n",
    "\n",
    "print(mlp_in_activations.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57664d1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 12, 768])\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "in_acts = einops.rearrange(mlp_in_activations, \"l b d -> b l d\")\n",
    "print(in_acts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3dd5db",
   "metadata": {},
   "source": [
    "### Encode w/o standarizer folding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8458d4d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e5b2b2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 12, 10000])\n",
      "sparse_features._nnz(): 70298\n",
      "l0_avg_per_layer: 836.8809814453125\n"
     ]
    }
   ],
   "source": [
    "features = clt_module.model.encode(in_acts)\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "sparse_features = features.to_sparse()\n",
    "print(f\"sparse_features._nnz(): {sparse_features._nnz()}\")\n",
    "\n",
    "l0_avg_per_layer = torch.count_nonzero(features > 0) / (\n",
    "    features.shape[0] * features.shape[1]\n",
    ")\n",
    "print(f\"l0_avg_per_layer: {l0_avg_per_layer.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d847db4",
   "metadata": {},
   "source": [
    "### Encode w/ folding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26589596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 12, 10000])\n",
      "sparse_features._nnz(): 17896\n",
      "l0_avg_per_layer: 213.04762268066406\n"
     ]
    }
   ],
   "source": [
    "\n",
    "features = clt_module.model.encode_with_standardizer_folding(in_acts)\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "sparse_features = features.to_sparse()\n",
    "print(f\"sparse_features._nnz(): {sparse_features._nnz()}\")\n",
    "\n",
    "l0_avg_per_layer = torch.count_nonzero(features > 0) / (features.shape[0] * features.shape[1])\n",
    "print(f\"l0_avg_per_layer: {l0_avg_per_layer.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130f0259",
   "metadata": {},
   "source": [
    "### Run encoding in bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "753de1ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 12, 10000])\n",
      "sparse_features._nnz(): 17843\n",
      "l0_avg_per_layer: 212.4166717529297\n"
     ]
    }
   ],
   "source": [
    "in_acts = in_acts.to(torch.bfloat16)\n",
    "clt_module.model.to(torch.bfloat16)\n",
    "features = clt_module.model.encode_with_standardizer_folding(in_acts)\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "sparse_features = features.to_sparse()\n",
    "print(f\"sparse_features._nnz(): {sparse_features._nnz()}\")\n",
    "\n",
    "l0_avg_per_layer = torch.count_nonzero(features > 0) / (\n",
    "    features.shape[0] * features.shape[1]\n",
    ")\n",
    "print(f\"l0_avg_per_layer: {l0_avg_per_layer.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca368d",
   "metadata": {},
   "source": [
    "## Interface w circuit tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ce09c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "from crosslayer_transcoder.utils.model_converters.circuit_tracer import (\n",
    "    CircuitTracerConverter,\n",
    ")\n",
    "from circuit_tracer.transcoder.cross_layer_transcoder import load_clt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c839fca",
   "metadata": {},
   "source": [
    "### Convert model to circuit-tracer format and save `.safetensors`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8c55f63b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting CLT : 100%|██████████| 12/12 [00:10<00:00,  1.13it/s]\n"
     ]
    }
   ],
   "source": [
    "save_dir = pathlib.Path(\"clt_module_test\")\n",
    "feature_input_hook = \"hook_resid_mid\"\n",
    "feature_output_hook = \"hook_mlp_out\"\n",
    "\n",
    "converter = CircuitTracerConverter(\n",
    "save_dir=save_dir,\n",
    "feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    ")\n",
    "converter.convert_and_save(clt_module, dtype=torch.bfloat16) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b91b8",
   "metadata": {},
   "source": [
    "### Load CLT into circuit-tracer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eb5ad901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['W_enc_0', 'b_dec_0', 'b_enc_0', 'threshold_0']\n",
      "['W_enc_1', 'b_dec_1', 'b_enc_1', 'threshold_1']\n",
      "['W_enc_2', 'b_dec_2', 'b_enc_2', 'threshold_2']\n",
      "['W_enc_3', 'b_dec_3', 'b_enc_3', 'threshold_3']\n",
      "['W_enc_4', 'b_dec_4', 'b_enc_4', 'threshold_4']\n",
      "['W_enc_5', 'b_dec_5', 'b_enc_5', 'threshold_5']\n",
      "['W_enc_6', 'b_dec_6', 'b_enc_6', 'threshold_6']\n",
      "['W_enc_7', 'b_dec_7', 'b_enc_7', 'threshold_7']\n",
      "['W_enc_8', 'b_dec_8', 'b_enc_8', 'threshold_8']\n",
      "['W_enc_9', 'b_dec_9', 'b_enc_9', 'threshold_9']\n",
      "['W_enc_10', 'b_dec_10', 'b_enc_10', 'threshold_10']\n",
      "['W_enc_11', 'b_dec_11', 'b_enc_11', 'threshold_11']\n",
      "dict_keys(['b_dec', 'b_enc', 'activation_function.threshold', 'W_enc', 'W_dec.0', 'W_dec.1', 'W_dec.2', 'W_dec.3', 'W_dec.4', 'W_dec.5', 'W_dec.6', 'W_dec.7', 'W_dec.8', 'W_dec.9', 'W_dec.10', 'W_dec.11'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "circuit_tracer_transcoder, state_dict_pre_load = load_clt(\n",
    "    clt_path=save_dir.as_posix(),\n",
    "    lazy_decoder=False,\n",
    "    lazy_encoder=False,\n",
    "    feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3df8f67",
   "metadata": {},
   "source": [
    "### Sanity checks for loaded model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "78ff27a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import einops\n",
    "from safetensors import safe_open\n",
    "import torch\n",
    "import os\n",
    "# sanity check against original model\n",
    "\n",
    "clt_path = save_dir.as_posix()\n",
    "assert circuit_tracer_transcoder.clt_path == clt_path\n",
    "\n",
    "\n",
    "# TEST: state_dict_pre_load weights match the files before being loaded into the model\n",
    "for i in range(circuit_tracer_transcoder.n_layers):\n",
    "    enc_file = os.path.join(clt_path, f\"W_enc_{i}.safetensors\")\n",
    "    with safe_open(enc_file, framework=\"pt\", device=circuit_tracer_transcoder.device.type) as f:\n",
    "        w_file = f.get_tensor(f\"W_enc_{i}\").to(\n",
    "            dtype=state_dict_pre_load[\"W_enc\"][i].dtype,\n",
    "            device=state_dict_pre_load[\"W_enc\"][i].device,\n",
    "        )\n",
    "        w_state = state_dict_pre_load[\"W_enc\"][i]\n",
    "        assert w_file.shape == w_state.shape, (\n",
    "            f\"W_enc_{i} shape mismatch: {w_file.shape} != {w_state.shape}\"\n",
    "        )\n",
    "        assert w_file.dtype == w_state.dtype, (\n",
    "            f\"W_enc_{i} dtype mismatch: {w_file.dtype} != {w_state.dtype}\"\n",
    "        )\n",
    "        assert torch.allclose(w_file, w_state), (\n",
    "            i,\n",
    "            (w_file - w_state).abs().max().item(),\n",
    "        )\n",
    "\n",
    "\n",
    "# TEST: weights in the files should equal weights from the circuit_tracer_transcoder\n",
    "for i in range(circuit_tracer_transcoder.n_layers):\n",
    "    w_model = circuit_tracer_transcoder._get_encoder_weights(i)  # works for both lazy and eager\n",
    "    enc_file = os.path.join(clt_path, f\"W_enc_{i}.safetensors\")\n",
    "    with safe_open(enc_file, framework=\"pt\", device=circuit_tracer_transcoder.device.type) as f:\n",
    "        w_file = f.get_tensor(f\"W_enc_{i}\").to(\n",
    "            dtype=w_model.dtype, device=w_model.device\n",
    "        )\n",
    "\n",
    "    assert w_model.shape == w_file.shape\n",
    "    assert w_model.dtype == w_file.dtype\n",
    "    assert torch.allclose(w_model, w_file), (i, (w_model - w_file).abs().max().item())\n",
    "\n",
    "\n",
    "# fold\n",
    "standardizer = clt_module.model.input_standardizer\n",
    "W_enc_folded, b_enc_folded = standardizer.fold_in_encoder(\n",
    "    clt_module.model.encoder.W.to(dtype=torch.bfloat16),\n",
    "    clt_module.model.encoder.b.to(dtype=torch.bfloat16),\n",
    ")\n",
    "\n",
    "W_enc_folded = W_enc_folded.to(dtype=torch.bfloat16)\n",
    "b_enc_folded = b_enc_folded.to(dtype=torch.bfloat16)\n",
    "\n",
    "state_dict = {}\n",
    "device = clt_module.device\n",
    "# TEST: eights in the file should equal the folded weights\n",
    "for i in range(clt_module.model.encoder.n_layers):\n",
    "    enc_file = f\"W_enc_{i}.safetensors\"\n",
    "    with safe_open(\n",
    "        os.path.join(clt_path, enc_file), framework=\"pt\", device=device.type\n",
    "    ) as f:\n",
    "        assert W_enc_folded[i].T.shape == f.get_tensor(f\"W_enc_{i}\").shape, (\n",
    "            f\"W_enc_{i} shape mismatch: {W_enc_folded[i].shape} != {f.get_tensor(f'W_enc_{i}').shape}\"\n",
    "        )\n",
    "        assert W_enc_folded[i].dtype == f.get_tensor(f\"W_enc_{i}\").dtype, (\n",
    "            f\"W_enc_{i} dtype mismatch: {W_enc_folded[i].dtype} != {f.get_tensor(f'W_enc_{i}').dtype}\"\n",
    "        )\n",
    "        assert torch.allclose(W_enc_folded[i].T, f.get_tensor(f\"W_enc_{i}\"))\n",
    "        assert torch.allclose(b_enc_folded[i], f.get_tensor(f\"b_enc_{i}\"))\n",
    "\n",
    "        # loaded model\n",
    "        assert circuit_tracer_transcoder.W_enc[i].shape == f.get_tensor(f\"W_enc_{i}\").shape\n",
    "        assert circuit_tracer_transcoder.W_enc[i].dtype == f.get_tensor(f\"W_enc_{i}\").dtype, (\n",
    "            f\"W_enc_{i} dtype mismatch: {circuit_tracer_transcoder.W_enc[i].dtype} != {f.get_tensor(f'W_enc_{i}').dtype}\"\n",
    "        )\n",
    "        assert torch.allclose(\n",
    "            circuit_tracer_transcoder.W_enc[i], f.get_tensor(f\"W_enc_{i}\").to(\"cuda:0\")\n",
    "        )\n",
    "\n",
    "\n",
    "for i in range(clt_module.model.encoder.n_layers):\n",
    "    rearranged_W_enc = einops.rearrange(\n",
    "        W_enc_folded[i],\n",
    "        \"d_acts d_features -> d_features d_acts\",\n",
    "    ).contiguous()\n",
    "    assert torch.allclose(\n",
    "        rearranged_W_enc.to(\"cuda:0\"), circuit_tracer_transcoder.W_enc[i].to(\"cuda:0\")\n",
    "    )\n",
    "    assert torch.allclose(\n",
    "        b_enc_folded[i].to(dtype=torch.bfloat16).to(\"cuda:0\"),\n",
    "        circuit_tracer_transcoder.b_enc[i].to(dtype=torch.bfloat16).to(\"cuda:0\"),\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
