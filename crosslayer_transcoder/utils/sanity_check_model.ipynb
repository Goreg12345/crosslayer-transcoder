{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e36e2f4",
   "metadata": {},
   "source": [
    "# Sanity check model\n",
    "\n",
    "Sanity check the model checkpoint by loading it into our own arch and then running a prompt through the encoder to see what the sparsity of the model is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "de7531d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {'class_path': 'crosslayer_transcoder.model.clt.CrossLayerTranscoder', 'init_args': {'encoder': {'class_path': 'crosslayer_transcoder.model.clt.Encoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}, 'decoder': {'class_path': 'crosslayer_transcoder.model.clt.CrosslayerDecoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}, 'nonlinearity': {'class_path': 'crosslayer_transcoder.model.jumprelu.JumpReLU', 'init_args': {'theta': 0.03, 'bandwidth': 0.01, 'n_layers': 12, 'd_features': 10000}}, 'input_standardizer': {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseInputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}, 'output_standardizer': {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseOutputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}}}\n",
      "encoder {'class_path': 'crosslayer_transcoder.model.clt.Encoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}\n",
      "d_acts 768\n",
      "d_features 10000\n",
      "n_layers 12\n",
      "decoder {'class_path': 'crosslayer_transcoder.model.clt.CrosslayerDecoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}\n",
      "d_acts 768\n",
      "d_features 10000\n",
      "n_layers 12\n",
      "nonlinearity {'class_path': 'crosslayer_transcoder.model.jumprelu.JumpReLU', 'init_args': {'theta': 0.03, 'bandwidth': 0.01, 'n_layers': 12, 'd_features': 10000}}\n",
      "theta 0.03\n",
      "bandwidth 0.01\n",
      "n_layers 12\n",
      "d_features 10000\n",
      "input_standardizer {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseInputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}\n",
      "n_layers 12\n",
      "activation_dim 768\n",
      "output_standardizer {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseOutputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}\n",
      "n_layers 12\n",
      "activation_dim 768\n",
      "replacement_model {'class_path': 'crosslayer_transcoder.metrics.replacement_model_accuracy.ReplacementModelAccuracy', 'init_args': {'model_name': 'openai-community/gpt2', 'device_map': 'cuda:0', 'loader_batch_size': 2}}\n",
      "model_name openai-community/gpt2\n",
      "device_map cuda:0\n",
      "loader_batch_size 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d361419e352494f9c94caa4397c8508",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead_features {'class_path': 'crosslayer_transcoder.metrics.dead_features.DeadFeatures', 'init_args': {'n_features': 10000, 'n_layers': 12, 'return_per_layer': True, 'return_log_freqs': True, 'return_neuron_indices': True}}\n",
      "n_features 10000\n",
      "n_layers 12\n",
      "return_per_layer True\n",
      "return_log_freqs True\n",
      "return_neuron_indices True\n",
      "learning_rate 3e-4\n",
      "compile True\n",
      "lr_decay_step 16000\n",
      "lr_decay_factor 0.1\n",
      "lambda_sparsity 0.0007\n",
      "c_sparsity 1\n",
      "use_tanh True\n",
      "pre_actv_loss 1e-6\n",
      "compute_dead_features True\n",
      "compute_dead_features_every 500\n",
      "JumpReLUCrossLayerTranscoderModule(\n",
      "  (model): CrossLayerTranscoder(\n",
      "    (encoder): Encoder()\n",
      "    (decoder): CrosslayerDecoder()\n",
      "    (nonlinearity): JumpReLU()\n",
      "    (input_standardizer): DimensionwiseInputStandardizer()\n",
      "    (output_standardizer): DimensionwiseOutputStandardizer()\n",
      "  )\n",
      "  (replacement_model): ReplacementModelAccuracy(\n",
      "    (replacement_model): ReplacementModel()\n",
      "  )\n",
      "  (dead_features): DeadFeatures()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from crosslayer_transcoder.utils.module_builder import build_module_from_config, yaml_to_config\n",
    "from crosslayer_transcoder.utils.checkpoints import load_model_from_lightning_checkpoint\n",
    "\n",
    "config = yaml_to_config(\"../../config/circuit-tracer.yaml\")\n",
    "clt_module = build_module_from_config(config)\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = \"../checkpoints/clt.ckpt\"\n",
    "\n",
    "clt_module = load_model_from_lightning_checkpoint(clt_module, checkpoint)\n",
    "\n",
    "print(clt_module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "597a9d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnsight import LanguageModel\n",
    "prompt = (\n",
    "    \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    ")\n",
    "llm = LanguageModel(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5030984",
   "metadata": {},
   "source": [
    "## Collect Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cda0fb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 7, 768])\n",
      "torch.Size([12, 7, 768])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "mlp_in_activations = []\n",
    "mlp_out_activations = []\n",
    "\n",
    "with llm.trace(prompt) as trace:\n",
    "    for layer in range(12):\n",
    "        layer_activations = llm.transformer.h[layer].ln_2.input.save()\n",
    "        layer_activations = llm.transformer.h[layer].mlp.output.save()\n",
    "        mlp_in_activations.append(layer_activations.squeeze(0))\n",
    "        mlp_out_activations.append(layer_activations.squeeze(0))\n",
    "\n",
    "mlp_in_activations = torch.stack(mlp_in_activations, dim=0)\n",
    "mlp_out_activations = torch.stack(mlp_out_activations, dim=0)\n",
    "\n",
    "print(mlp_in_activations.shape)\n",
    "print(mlp_out_activations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d847db4",
   "metadata": {},
   "source": [
    "## Run Encoder w activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26589596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7, 12, 768])\n",
      "torch.Size([7, 12, 768])\n",
      "torch.Size([7, 2, 12, 768])\n",
      "torch.Size([7, 12, 10000])\n",
      "torch.Size([7, 12, 10000])\n",
      "4162\n",
      "49.5476188659668\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "\n",
    "in_acts = einops.rearrange(mlp_in_activations, \"l b d -> b l d\")\n",
    "out_acts = einops.rearrange(mlp_out_activations, \"l b d -> b l d\")\n",
    "print(in_acts.shape)\n",
    "print(out_acts.shape)\n",
    "batch_acts = torch.stack([in_acts, out_acts], dim=1)\n",
    "print(batch_acts.shape)\n",
    "clt_module.model.initialize_standardizers(batch_acts)\n",
    "_, features, _, _ = clt_module.model(in_acts)\n",
    "\n",
    "print(features.shape)\n",
    "\n",
    "sparse_features = features.to_sparse()\n",
    "print(sparse_features._nnz())\n",
    "\n",
    "l0_avg_per_layer = torch.count_nonzero(features > 0) / (features.shape[0] * features.shape[1])\n",
    "print(l0_avg_per_layer.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee75a5df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdaa188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
