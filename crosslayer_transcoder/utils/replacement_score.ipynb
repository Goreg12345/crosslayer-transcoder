{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc2a0d5",
   "metadata": {},
   "source": [
    "# Replacement Score Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19aa81f",
   "metadata": {},
   "source": [
    "## Load the model from the local checkpoint (save from HF before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90bb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from crosslayer_transcoder.utils.model_converters.circuit_tracer import (\n",
    "    CircuitTracerConverter,\n",
    ")\n",
    "from transformer_lens.loading_from_pretrained import get_pretrained_model_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c166ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a327355ed64e53a3904d6c23fd3796",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Converting CLT : 100%|██████████| 12/12 [00:24<00:00,  2.01s/it]\n"
     ]
    }
   ],
   "source": [
    "from crosslayer_transcoder.utils.module_builder import build_module_from_config, yaml_to_config\n",
    "from crosslayer_transcoder.utils.checkpoints import load_model_from_lightning_checkpoint\n",
    "\n",
    "config = yaml_to_config(\"../../config/circuit-tracer.yaml\")\n",
    "clt_module = build_module_from_config(config)\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = \"../checkpoints/clt.ckpt\"\n",
    "\n",
    "clt_module = load_model_from_lightning_checkpoint(clt_module, checkpoint)\n",
    "\n",
    "\n",
    "save_dir = pathlib.Path(\"clt_module_test\")\n",
    "feature_input_hook = \"hook_resid_mid\"\n",
    "feature_output_hook = \"hook_mlp_out\"\n",
    "\n",
    "converter = CircuitTracerConverter(\n",
    "save_dir=save_dir,\n",
    "feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    ")\n",
    "converter.convert_and_save(clt_module, dtype=torch.bfloat16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d8594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['W_enc_0', 'b_dec_0', 'b_enc_0', 'threshold_0']\n",
      "['W_enc_1', 'b_dec_1', 'b_enc_1', 'threshold_1']\n",
      "['W_enc_2', 'b_dec_2', 'b_enc_2', 'threshold_2']\n",
      "['W_enc_3', 'b_dec_3', 'b_enc_3', 'threshold_3']\n",
      "['W_enc_4', 'b_dec_4', 'b_enc_4', 'threshold_4']\n",
      "['W_enc_5', 'b_dec_5', 'b_enc_5', 'threshold_5']\n",
      "['W_enc_6', 'b_dec_6', 'b_enc_6', 'threshold_6']\n",
      "['W_enc_7', 'b_dec_7', 'b_enc_7', 'threshold_7']\n",
      "['W_enc_8', 'b_dec_8', 'b_enc_8', 'threshold_8']\n",
      "['W_enc_9', 'b_dec_9', 'b_enc_9', 'threshold_9']\n",
      "['W_enc_10', 'b_dec_10', 'b_enc_10', 'threshold_10']\n",
      "['W_enc_11', 'b_dec_11', 'b_enc_11', 'threshold_11']\n",
      "dict_keys(['b_dec', 'b_enc', 'activation_function.threshold', 'W_enc', 'W_dec.0', 'W_dec.1', 'W_dec.2', 'W_dec.3', 'W_dec.4', 'W_dec.5', 'W_dec.6', 'W_dec.7', 'W_dec.8', 'W_dec.9', 'W_dec.10', 'W_dec.11'])\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.transcoder.cross_layer_transcoder import load_clt\n",
    "transcoder, state_dict_pre_load = load_clt(\n",
    "    clt_path=save_dir.as_posix(),\n",
    "    lazy_decoder=False,\n",
    "    lazy_encoder=False,\n",
    "    feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bcce2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer import ReplacementModel\n",
    "\n",
    "\n",
    "config = get_pretrained_model_config(\"gpt2\")\n",
    "config.d_type = torch.float16\n",
    "rm = ReplacementModel.from_config(\n",
    "    config=config,\n",
    "    transcoders=transcoder,\n",
    ")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10269291",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    ")\n",
    "max_n_logits = 10  # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 100  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size = 256//8  # Batch size when attributing\n",
    "offload = (\n",
    "    \"cpu\"\n",
    ")  # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb6f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: torch.Size([8])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "Features shape: torch.Size([12, 8, 10000])\n",
      "Encoder vectors shape: torch.Size([840000, 768])\n",
      "nnz: 840000\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 44.34 GiB of which 10.55 GiB is free. Process 3607992 has 8.54 GiB memory in use. Process 3613770 has 25.24 GiB memory in use. Of the allocated memory 23.21 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# FOr some reason this takes up a lot of VRAM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m graph = \u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/attribution/attribute.py:137\u001b[39m, in \u001b[36mattribute\u001b[39m\u001b[34m(prompt, model, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, update_interval)\u001b[39m\n\u001b[32m    135\u001b[39m offload_handles = []\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m reload_handle \u001b[38;5;129;01min\u001b[39;00m offload_handles:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/attribution/attribute.py:177\u001b[39m, in \u001b[36m_run_attribution\u001b[39m\u001b[34m(model, prompt, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, offload_handles, logger, update_interval)\u001b[39m\n\u001b[32m    174\u001b[39m phase_start = time.time()\n\u001b[32m    175\u001b[39m input_ids = model.ensure_tokenized(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m ctx = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_attribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m activation_matrix = ctx.activation_matrix\n\u001b[32m    180\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecomputation completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mphase_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/crosslayer-transcoder/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/replacement_model.py:435\u001b[39m, in \u001b[36mReplacementModel.setup_attribution\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    432\u001b[39m mlp_in_cache = torch.cat(\u001b[38;5;28mlist\u001b[39m(mlp_in_cache.values()), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    433\u001b[39m mlp_out_cache = torch.cat(\u001b[38;5;28mlist\u001b[39m(mlp_out_cache.values()), dim=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m attribution_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranscoders\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_attribution_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_in_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Compute error vectors\u001b[39;00m\n\u001b[32m    438\u001b[39m error_vectors = mlp_out_cache - attribution_data[\u001b[33m\"\u001b[39m\u001b[33mreconstruction\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/transcoder/cross_layer_transcoder.py:313\u001b[39m, in \u001b[36mCrossLayerTranscoder.compute_attribution_components\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncoder vectors shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_vectors.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    311\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnnz: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures._nnz()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    312\u001b[39m pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_to_decoder_map = (\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_decoder_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m )\n\u001b[32m    315\u001b[39m \u001b[38;5;28mprint\u001b[39m(features.shape)\n\u001b[32m    316\u001b[39m reconstruction = \u001b[38;5;28mself\u001b[39m.compute_reconstruction(pos_ids, layer_ids, decoder_vectors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/transcoder/cross_layer_transcoder.py:270\u001b[39m, in \u001b[36mCrossLayerTranscoder.select_decoder_vectors\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    268\u001b[39m layer_ids = torch.cat(layer_ids, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    269\u001b[39m feat_ids = torch.cat(feat_ids, dim=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m decoder_vectors = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m encoder_mapping = torch.cat(encoder_mapping, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_mapping\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 44.34 GiB of which 10.55 GiB is free. Process 3607992 has 8.54 GiB memory in use. Process 3613770 has 25.24 GiB memory in use. Of the allocated memory 23.21 GiB is allocated by PyTorch, and 1.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# FOr some reason this takes up a lot of VRAM\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=rm,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
