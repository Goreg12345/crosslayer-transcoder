{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc2a0d5",
   "metadata": {},
   "source": [
    "# Compute Graph Replacement Score & Visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a091f9ee",
   "metadata": {},
   "source": [
    "### Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37586eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "huggingface_path = \"georglange/crosslayer-transcoder-topk-16k\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4458999",
   "metadata": {},
   "source": [
    "### Load model from HuggingFace "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c166ce94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea4d424e20614d70823decee70cbdc5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from crosslayer_transcoder.model.serializable_module import SerializableModule\n",
    "\n",
    "\n",
    "clt_model = SerializableModule.from_pretrained(huggingface_path)\n",
    "assert clt_model._is_folded "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95a146b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crosslayer_transcoder.model.clt import CrossLayerTranscoder\n",
    "assert isinstance(clt_model, CrossLayerTranscoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f259397a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting CLT encoder: 100%|██████████| 12/12 [00:33<00:00,  2.79s/it]\n"
     ]
    }
   ],
   "source": [
    "from crosslayer_transcoder.utils.model_converters.circuit_tracer import (\n",
    "    CircuitTracerConverter,\n",
    ")\n",
    "save_dir = \"circuit_tracing_replacement_score\"\n",
    "feature_input_hook = \"hook_resid_mid\"\n",
    "feature_output_hook = \"hook_mlp_out\"\n",
    "\n",
    "converter = CircuitTracerConverter(\n",
    "save_dir=save_dir,\n",
    "feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    ")\n",
    "converter.convert_and_save(clt_model) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae50d526",
   "metadata": {},
   "source": [
    "### Load model from local converted checkpoint\n",
    "\n",
    "In the future, this could be loaded from huggingface using the `ReplacementModel.from_pretrained`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8d8594b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from circuit_tracer.transcoder.cross_layer_transcoder import load_clt\n",
    "scan_id = \"dallas-austin\"\n",
    "circuit_tracer_clt = load_clt(\n",
    "    clt_path=save_dir,\n",
    "    lazy_decoder=False,\n",
    "    lazy_encoder=False,\n",
    "    feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    "    scan=scan_id\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bcce2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2 into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer import ReplacementModel\n",
    "\n",
    "rm = ReplacementModel.from_pretrained_and_transcoders(\n",
    "    \"gpt2\",\n",
    "    circuit_tracer_clt,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8448dc8b",
   "metadata": {},
   "source": [
    "### Attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10269291",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"Mary and John went to the store, Jon gave a drink to\"  # What you want to get the graph for\n",
    ")\n",
    "max_n_logits = 10  # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 8192  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size = 256  # Batch size when attributing\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ecb6f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n",
      "Precomputation completed in 0.38s\n",
      "Found 2496 active features\n",
      "Phase 1: Running forward pass\n",
      "Forward pass completed in 0.03s\n",
      "Phase 2: Building input vectors\n",
      "Selected 10 logits with cumulative probability 0.7510\n",
      "Will include 2496 of 2496 feature nodes\n",
      "Input vectors built in 0.04s\n",
      "Phase 3: Computing logit attributions\n",
      "sys:1: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "Logit attributions completed in 0.14s\n",
      "Phase 4: Computing feature attributions\n",
      "Feature influence computation: 100%|██████████| 2496/2496 [00:01<00:00, 2007.91it/s]\n",
      "Feature attributions completed in 1.32s\n",
      "Attribution completed in 2.25s\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=rm,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=None,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa0b05",
   "metadata": {},
   "source": [
    "### Replacement Score\n",
    "\n",
    "Now that we have a graph created for our prompt above, we can generate metrics that tell us how much of the Replacement Model's computation is attributable to the feature vs error nodes. \n",
    "\n",
    "[Full explanation](https://transformer-circuits.pub/2025/attribution-graphs/methods.html#evaluating-graphs-comparing)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4008f230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "replacement score, completeness score\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7058860659599304, 0.9123725891113281)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from circuit_tracer.graph import compute_graph_scores\n",
    "print(\"replacement score, completeness score\")\n",
    "compute_graph_scores(graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c5e46",
   "metadata": {},
   "source": [
    "### [Optional] Visualize the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "191aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9e4da199",
   "metadata": {},
   "outputs": [],
   "source": [
    "slug = \"mary-john-store\"  # this is the name that you assign to the graph\n",
    "graph_file_dir = \"./graph_files\"  # where to write the graph files. no need to make this one; create_graph_files does that for you\n",
    "node_threshold = (\n",
    "    0.8  # keep only the minimum # of nodes whose cumulative influence is >= 0.8\n",
    ")\n",
    "edge_threshold = (\n",
    "    0.98  # keep only the minimum # of edges whose cumulative influence is >= 0.98\n",
    ")\n",
    "\n",
    "create_graph_files(\n",
    "    graph_or_path=graph,  # the graph to create files for\n",
    "    slug=slug,\n",
    "    output_path=graph_file_dir,\n",
    "    node_threshold=node_threshold,\n",
    "    edge_threshold=edge_threshold,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50185332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Open your graph here: f'http://localhost:8048/index.html'\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.frontend.local_server import serve\n",
    "from IPython.display import IFrame\n",
    "\n",
    "port = 8048\n",
    "server = serve(data_dir=\"./graph_files/\", port=port)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Open your graph here: f'http://localhost:{port}/index.html'\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
