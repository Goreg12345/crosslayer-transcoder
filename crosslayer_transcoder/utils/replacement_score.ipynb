{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1dc2a0d5",
   "metadata": {},
   "source": [
    "# Replacement Score Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19aa81f",
   "metadata": {},
   "source": [
    "## Load the model from the local checkpoint (save from HF before)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b90bb04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from crosslayer_transcoder.utils.model_converters.circuit_tracer import (\n",
    "    CircuitTracerConverter,\n",
    ")\n",
    "from transformer_lens.loading_from_pretrained import get_pretrained_model_config\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c166ce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model {'class_path': 'crosslayer_transcoder.model.clt.CrossLayerTranscoder', 'init_args': {'encoder': {'class_path': 'crosslayer_transcoder.model.clt.Encoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}, 'decoder': {'class_path': 'crosslayer_transcoder.model.clt.CrosslayerDecoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}, 'nonlinearity': {'class_path': 'crosslayer_transcoder.model.jumprelu.JumpReLU', 'init_args': {'theta': 0.03, 'bandwidth': 0.01, 'n_layers': 12, 'd_features': 10000}}, 'input_standardizer': {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseInputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}, 'output_standardizer': {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseOutputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}}}\n",
      "encoder {'class_path': 'crosslayer_transcoder.model.clt.Encoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}\n",
      "d_acts 768\n",
      "d_features 10000\n",
      "n_layers 12\n",
      "decoder {'class_path': 'crosslayer_transcoder.model.clt.CrosslayerDecoder', 'init_args': {'d_acts': 768, 'd_features': 10000, 'n_layers': 12}}\n",
      "d_acts 768\n",
      "d_features 10000\n",
      "n_layers 12\n",
      "nonlinearity {'class_path': 'crosslayer_transcoder.model.jumprelu.JumpReLU', 'init_args': {'theta': 0.03, 'bandwidth': 0.01, 'n_layers': 12, 'd_features': 10000}}\n",
      "theta 0.03\n",
      "bandwidth 0.01\n",
      "n_layers 12\n",
      "d_features 10000\n",
      "input_standardizer {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseInputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}\n",
      "n_layers 12\n",
      "activation_dim 768\n",
      "output_standardizer {'class_path': 'crosslayer_transcoder.model.standardize.DimensionwiseOutputStandardizer', 'init_args': {'n_layers': 12, 'activation_dim': 768}}\n",
      "n_layers 12\n",
      "activation_dim 768\n",
      "replacement_model {'class_path': 'crosslayer_transcoder.metrics.replacement_model_accuracy.ReplacementModelAccuracy', 'init_args': {'model_name': 'openai-community/gpt2', 'device_map': 'cuda:0', 'loader_batch_size': 2}}\n",
      "model_name openai-community/gpt2\n",
      "device_map cuda:0\n",
      "loader_batch_size 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9409e1c34a4ae3b2bac2bf5299dfe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1217 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1561 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1174 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2459 > 1024). Running this sequence through the model will result in indexing errors\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2027 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dead_features {'class_path': 'crosslayer_transcoder.metrics.dead_features.DeadFeatures', 'init_args': {'n_features': 10000, 'n_layers': 12, 'return_per_layer': True, 'return_log_freqs': True, 'return_neuron_indices': True}}\n",
      "n_features 10000\n",
      "n_layers 12\n",
      "return_per_layer True\n",
      "return_log_freqs True\n",
      "return_neuron_indices True\n",
      "learning_rate 3e-4\n",
      "compile True\n",
      "lr_decay_step 16000\n",
      "lr_decay_factor 0.1\n",
      "lambda_sparsity 0.0007\n",
      "c_sparsity 1\n",
      "use_tanh True\n",
      "pre_actv_loss 1e-6\n",
      "compute_dead_features True\n",
      "compute_dead_features_every 500\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n",
      "JumpReLU()\n",
      "torch.Size([1, 12, 10000])\n"
     ]
    }
   ],
   "source": [
    "from crosslayer_transcoder.utils.module_builder import build_module_from_config, yaml_to_config\n",
    "from crosslayer_transcoder.utils.checkpoints import load_model_from_lightning_checkpoint\n",
    "\n",
    "config = yaml_to_config(\"../../config/circuit-tracer.yaml\")\n",
    "clt_module = build_module_from_config(config)\n",
    "\n",
    "# load checkpoint\n",
    "checkpoint = \"../checkpoints/clt.ckpt\"\n",
    "\n",
    "clt_module = load_model_from_lightning_checkpoint(clt_module, checkpoint)\n",
    "\n",
    "\n",
    "save_dir = pathlib.Path(\"clt_module_test\")\n",
    "feature_input_hook = \"hook_resid_mid\"\n",
    "feature_output_hook = \"hook_mlp_out\"\n",
    "\n",
    "converter = CircuitTracerConverter(\n",
    "save_dir=save_dir,\n",
    "feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    ")\n",
    "converter.convert_and_save(clt_module, dtype=torch.bfloat16) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8d8594b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n",
      "['W_enc_0', 'b_dec_0', 'b_enc_0', 'threshold_0']\n",
      "['W_enc_1', 'b_dec_1', 'b_enc_1', 'threshold_1']\n",
      "['W_enc_2', 'b_dec_2', 'b_enc_2', 'threshold_2']\n",
      "['W_enc_3', 'b_dec_3', 'b_enc_3', 'threshold_3']\n",
      "['W_enc_4', 'b_dec_4', 'b_enc_4', 'threshold_4']\n",
      "['W_enc_5', 'b_dec_5', 'b_enc_5', 'threshold_5']\n",
      "['W_enc_6', 'b_dec_6', 'b_enc_6', 'threshold_6']\n",
      "['W_enc_7', 'b_dec_7', 'b_enc_7', 'threshold_7']\n",
      "['W_enc_8', 'b_dec_8', 'b_enc_8', 'threshold_8']\n",
      "['W_enc_9', 'b_dec_9', 'b_enc_9', 'threshold_9']\n",
      "['W_enc_10', 'b_dec_10', 'b_enc_10', 'threshold_10']\n",
      "['W_enc_11', 'b_dec_11', 'b_enc_11', 'threshold_11']\n",
      "dict_keys(['b_dec', 'b_enc', 'activation_function.threshold', 'W_enc', 'W_dec.0', 'W_dec.1', 'W_dec.2', 'W_dec.3', 'W_dec.4', 'W_dec.5', 'W_dec.6', 'W_dec.7', 'W_dec.8', 'W_dec.9', 'W_dec.10', 'W_dec.11'])\n",
      "JumpReLU\n",
      "W_enc\n",
      "torch.Size([12, 10000, 768])\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer.transcoder.cross_layer_transcoder import load_clt\n",
    "transcoder, state_dict_pre_load = load_clt(\n",
    "    clt_path=save_dir.as_posix(),\n",
    "    lazy_decoder=False,\n",
    "    lazy_encoder=False,\n",
    "    feature_input_hook=feature_input_hook,\n",
    "    feature_output_hook=feature_output_hook,\n",
    "    dtype=torch.bfloat16,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5ec0f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clt_module_test\n"
     ]
    }
   ],
   "source": [
    "import einops\n",
    "from safetensors import safe_open\n",
    "import torch\n",
    "import os\n",
    "# sanity check against original model \n",
    "\n",
    "print(transcoder.clt_path)\n",
    "clt_path = save_dir.as_posix()\n",
    "assert transcoder.clt_path == clt_path\n",
    "\n",
    "# for i in range(clt_module.model.encoder.n_layers):\n",
    "#     # THESE SHOULD NOT be close\n",
    "#     assert not torch.allclose(clt_module.model.encoder.W[i].T.to('cuda:0'), transcoder.W_enc[i].to('cuda:0')) # should fail bc of folding \n",
    "\n",
    "# TEST: state_dict_pre_load weights match the files before being loaded into the model\n",
    "for i in range(transcoder.n_layers):\n",
    "    enc_file = os.path.join(clt_path, f\"W_enc_{i}.safetensors\")\n",
    "    with safe_open(enc_file, framework=\"pt\", device=transcoder.device.type) as f:\n",
    "        w_file = f.get_tensor(f\"W_enc_{i}\").to(\n",
    "            dtype=state_dict_pre_load[\"W_enc\"][i].dtype, device=state_dict_pre_load[\"W_enc\"][i].device\n",
    "        )\n",
    "        w_state = state_dict_pre_load[\"W_enc\"][i]\n",
    "        assert w_file.shape == w_state.shape, f\"W_enc_{i} shape mismatch: {w_file.shape} != {w_state.shape}\"\n",
    "        assert w_file.dtype == w_state.dtype, f\"W_enc_{i} dtype mismatch: {w_file.dtype} != {w_state.dtype}\"\n",
    "        assert torch.allclose(w_file, w_state), (\n",
    "            i, (w_file - w_state).abs().max().item()\n",
    "        )\n",
    "\n",
    "\n",
    "# TEST: weights in the files should equal weights from the transcoder\n",
    "for i in range(transcoder.n_layers):\n",
    "    w_model = transcoder._get_encoder_weights(i)  # works for both lazy and eager\n",
    "    enc_file = os.path.join(clt_path, f\"W_enc_{i}.safetensors\")\n",
    "    with safe_open(enc_file, framework=\"pt\", device=transcoder.device.type) as f:\n",
    "        w_file = f.get_tensor(f\"W_enc_{i}\").to(\n",
    "            dtype=w_model.dtype, device=w_model.device\n",
    "        )\n",
    "\n",
    "    assert w_model.shape == w_file.shape\n",
    "    assert w_model.dtype == w_file.dtype\n",
    "    assert torch.allclose(w_model, w_file), (i, (w_model - w_file).abs().max().item())\n",
    "\n",
    "\n",
    "# fold\n",
    "standardizer = clt_module.model.input_standardizer\n",
    "W_enc_folded, b_enc_folded = standardizer.fold_in_encoder(clt_module.model.encoder.W.to(dtype=torch.bfloat16), clt_module.model.encoder.b.to(dtype=torch.bfloat16))\n",
    "\n",
    "W_enc_folded = W_enc_folded.to(dtype=torch.bfloat16)\n",
    "b_enc_folded = b_enc_folded.to(dtype=torch.bfloat16)\n",
    "\n",
    "state_dict = {}\n",
    "device = clt_module.device \n",
    "# TEST: eights in the file should equal the folded weights\n",
    "for i in range(clt_module.model.encoder.n_layers):\n",
    "    enc_file = f\"W_enc_{i}.safetensors\"\n",
    "    with safe_open(os.path.join(clt_path, enc_file), framework=\"pt\", device=device.type) as f:\n",
    "        assert W_enc_folded[i].T.shape == f.get_tensor(f\"W_enc_{i}\").shape, f\"W_enc_{i} shape mismatch: {W_enc_folded[i].shape} != {f.get_tensor(f'W_enc_{i}').shape}\"\n",
    "        assert W_enc_folded[i].dtype == f.get_tensor(f\"W_enc_{i}\").dtype, f\"W_enc_{i} dtype mismatch: {W_enc_folded[i].dtype} != {f.get_tensor(f'W_enc_{i}').dtype}\"\n",
    "        assert torch.allclose(W_enc_folded[i].T, f.get_tensor(f\"W_enc_{i}\"))\n",
    "        assert torch.allclose(b_enc_folded[i], f.get_tensor(f\"b_enc_{i}\"))\n",
    "\n",
    "\n",
    "        # loaded model \n",
    "        assert transcoder.W_enc[i].shape == f.get_tensor(f\"W_enc_{i}\").shape\n",
    "        assert transcoder.W_enc[i].dtype == f.get_tensor(f\"W_enc_{i}\").dtype, f\"W_enc_{i} dtype mismatch: {transcoder.W_enc[i].dtype} != {f.get_tensor(f'W_enc_{i}').dtype}\"\n",
    "        assert torch.allclose(transcoder.W_enc[i], f.get_tensor(f\"W_enc_{i}\").to(\"cuda:0\"))\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "for i in range(clt_module.model.encoder.n_layers):\n",
    "    rearranged_W_enc = einops.rearrange(\n",
    "        W_enc_folded[i],\n",
    "        \"d_acts d_features -> d_features d_acts\",\n",
    "    ).contiguous()\n",
    "    assert torch.allclose(rearranged_W_enc.to('cuda:0'), transcoder.W_enc[i].to('cuda:0'))\n",
    "    assert torch.allclose(b_enc_folded[i].to(dtype=torch.bfloat16).to('cuda:0'),transcoder.b_enc[i].to(dtype=torch.bfloat16).to('cuda:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1bcce2aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossLayerTranscoder(\n",
      "  (activation_function): JumpReLU(\n",
      "    threshold=Parameter containing:\n",
      "    tensor([[[0.2344, 0.2080, 0.2246,  ..., 0.2207, 0.2363, 0.2314]],\n",
      "    \n",
      "            [[0.2451, 0.2334, 0.2578,  ..., 0.2676, 0.2432, 0.2256]],\n",
      "    \n",
      "            [[0.2402, 0.2363, 0.2402,  ..., 0.2139, 0.2520, 0.2002]],\n",
      "    \n",
      "            ...,\n",
      "    \n",
      "            [[0.2168, 0.3789, 0.2480,  ..., 0.2305, 0.3242, 0.2793]],\n",
      "    \n",
      "            [[0.2891, 0.2158, 0.1943,  ..., 0.2793, 0.2812, 0.2949]],\n",
      "    \n",
      "            [[0.1934, 0.1543, 0.1758,  ..., 0.2832, 0.2324, 0.2197]]],\n",
      "           device='cuda:0'), bandwidth=2\n",
      "  )\n",
      "  (W_dec): ParameterList(\n",
      "      (0): Parameter containing: [torch.float32 of size 10000x12x768 (cuda:0)]\n",
      "      (1): Parameter containing: [torch.float32 of size 10000x11x768 (cuda:0)]\n",
      "      (2): Parameter containing: [torch.float32 of size 10000x10x768 (cuda:0)]\n",
      "      (3): Parameter containing: [torch.float32 of size 10000x9x768 (cuda:0)]\n",
      "      (4): Parameter containing: [torch.float32 of size 10000x8x768 (cuda:0)]\n",
      "      (5): Parameter containing: [torch.float32 of size 10000x7x768 (cuda:0)]\n",
      "      (6): Parameter containing: [torch.float32 of size 10000x6x768 (cuda:0)]\n",
      "      (7): Parameter containing: [torch.float32 of size 10000x5x768 (cuda:0)]\n",
      "      (8): Parameter containing: [torch.float32 of size 10000x4x768 (cuda:0)]\n",
      "      (9): Parameter containing: [torch.float32 of size 10000x3x768 (cuda:0)]\n",
      "      (10): Parameter containing: [torch.float32 of size 10000x2x768 (cuda:0)]\n",
      "      (11): Parameter containing: [torch.float32 of size 10000x1x768 (cuda:0)]\n",
      "  )\n",
      ")\n",
      "torch.Size([12, 10000, 768])\n",
      "[torch.Size([12, 10000, 768]), torch.Size([12, 768]), torch.Size([12, 10000]), torch.Size([12, 1, 10000]), torch.Size([10000, 12, 768]), torch.Size([10000, 11, 768]), torch.Size([10000, 10, 768]), torch.Size([10000, 9, 768]), torch.Size([10000, 8, 768]), torch.Size([10000, 7, 768]), torch.Size([10000, 6, 768]), torch.Size([10000, 5, 768]), torch.Size([10000, 4, 768]), torch.Size([10000, 3, 768]), torch.Size([10000, 2, 768]), torch.Size([10000, 1, 768])]\n",
      "JumpReLU(\n",
      "  threshold=Parameter containing:\n",
      "  tensor([[[0.2344, 0.2080, 0.2246,  ..., 0.2207, 0.2363, 0.2314]],\n",
      "  \n",
      "          [[0.2451, 0.2334, 0.2578,  ..., 0.2676, 0.2432, 0.2256]],\n",
      "  \n",
      "          [[0.2402, 0.2363, 0.2402,  ..., 0.2139, 0.2520, 0.2002]],\n",
      "  \n",
      "          ...,\n",
      "  \n",
      "          [[0.2168, 0.3789, 0.2480,  ..., 0.2305, 0.3242, 0.2793]],\n",
      "  \n",
      "          [[0.2891, 0.2158, 0.1943,  ..., 0.2793, 0.2812, 0.2949]],\n",
      "  \n",
      "          [[0.1934, 0.1543, 0.1758,  ..., 0.2832, 0.2324, 0.2197]]],\n",
      "         device='cuda:0'), bandwidth=2\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from circuit_tracer import ReplacementModel\n",
    "\n",
    "\n",
    "config = get_pretrained_model_config(\"gpt2\")\n",
    "config.d_type = torch.float16\n",
    "rm = ReplacementModel.from_config(\n",
    "    config=config,\n",
    "    transcoders=transcoder,\n",
    ")\n",
    "\n",
    "\n",
    "print(rm.transcoders)\n",
    "print(rm.transcoders.W_enc.shape)\n",
    "assert torch.allclose(rm.transcoders.W_enc, transcoder.W_enc)\n",
    "print([x.shape for x in list(rm.transcoders.parameters())])\n",
    "print(rm.transcoders.activation_function)\n",
    "\n",
    "## TODO: sanity check model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10269291",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = (\n",
    "    \"The capital of state containing Dallas is\"  # What you want to get the graph for\n",
    ")\n",
    "max_n_logits = 10  # How many logits to attribute from, max. We attribute to min(max_n_logits, n_logits_to_reach_desired_log_prob); see below for the latter\n",
    "desired_logit_prob = 0.95  # Attribution will attribute from the minimum number of logits needed to reach this probability mass (or max_n_logits, whichever is lower)\n",
    "max_feature_nodes = 100  # Only attribute from this number of feature nodes, max. Lower is faster, but you will lose more of the graph. None means no limit.\n",
    "batch_size = 256//8  # Batch size when attributing\n",
    "offload = (\n",
    "    \"cpu\"\n",
    ")  # Offload various parts of the model during attribution to save memory. Can be 'disk', 'cpu', or None (keep on GPU)\n",
    "verbose = True  # Whether to display a tqdm progress bar and timing report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ecb6f82c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Phase 0: Precomputing activations and vectors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens shape: torch.Size([8])\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "W_enc_layer shape: torch.Size([10000, 768])\n",
      "sparse_W_enc_layer nnz: 7680000\n",
      "layer_features shape: torch.Size([8, 10000])\n",
      "sparse_pre_activations nnz: 80000\n",
      "layer_features shape after activation: torch.Size([8, 10000])\n",
      "l0: 70000\n",
      "sparse_layer nnz: 70000\n",
      "Features shape: torch.Size([12, 8, 10000])\n",
      "Encoder vectors shape: torch.Size([840000, 768])\n",
      "nnz: 840000\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 44.34 GiB of which 10.35 GiB is free. Process 2491674 has 8.54 GiB memory in use. Process 2496736 has 25.44 GiB memory in use. Of the allocated memory 23.23 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m torch.cuda.empty_cache()\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# FOr some reason this takes up a lot of VRAM\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m graph = \u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/attribution/attribute.py:137\u001b[39m, in \u001b[36mattribute\u001b[39m\u001b[34m(prompt, model, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, update_interval)\u001b[39m\n\u001b[32m    135\u001b[39m offload_handles = []\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_run_attribution\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    139\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    140\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_n_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdesired_logit_prob\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    143\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_feature_nodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m        \u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m=\u001b[49m\u001b[43moffload_handles\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlogger\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    151\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m reload_handle \u001b[38;5;129;01min\u001b[39;00m offload_handles:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/attribution/attribute.py:177\u001b[39m, in \u001b[36m_run_attribution\u001b[39m\u001b[34m(model, prompt, max_n_logits, desired_logit_prob, batch_size, max_feature_nodes, offload, verbose, offload_handles, logger, update_interval)\u001b[39m\n\u001b[32m    174\u001b[39m phase_start = time.time()\n\u001b[32m    175\u001b[39m input_ids = model.ensure_tokenized(prompt)\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m ctx = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msetup_attribution\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m activation_matrix = ctx.activation_matrix\n\u001b[32m    180\u001b[39m logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mPrecomputation completed in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime.time()\u001b[38;5;250m \u001b[39m-\u001b[38;5;250m \u001b[39mphase_start\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/crosslayer-transcoder/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/replacement_model.py:435\u001b[39m, in \u001b[36mReplacementModel.setup_attribution\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    432\u001b[39m mlp_in_cache = torch.cat(\u001b[38;5;28mlist\u001b[39m(mlp_in_cache.values()), dim=\u001b[32m0\u001b[39m)\n\u001b[32m    433\u001b[39m mlp_out_cache = torch.cat(\u001b[38;5;28mlist\u001b[39m(mlp_out_cache.values()), dim=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m435\u001b[39m attribution_data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtranscoders\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_attribution_components\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmlp_in_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m \u001b[38;5;66;03m# Compute error vectors\u001b[39;00m\n\u001b[32m    438\u001b[39m error_vectors = mlp_out_cache - attribution_data[\u001b[33m\"\u001b[39m\u001b[33mreconstruction\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/transcoder/cross_layer_transcoder.py:323\u001b[39m, in \u001b[36mCrossLayerTranscoder.compute_attribution_components\u001b[39m\u001b[34m(self, inputs)\u001b[39m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEncoder vectors shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mencoder_vectors.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    321\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mnnz: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeatures._nnz()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    322\u001b[39m pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_to_decoder_map = (\n\u001b[32m--> \u001b[39m\u001b[32m323\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mselect_decoder_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    324\u001b[39m )\n\u001b[32m    325\u001b[39m \u001b[38;5;28mprint\u001b[39m(features.shape)\n\u001b[32m    326\u001b[39m reconstruction = \u001b[38;5;28mself\u001b[39m.compute_reconstruction(pos_ids, layer_ids, decoder_vectors)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/circuit-tracer/circuit_tracer/transcoder/cross_layer_transcoder.py:280\u001b[39m, in \u001b[36mCrossLayerTranscoder.select_decoder_vectors\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    278\u001b[39m layer_ids = torch.cat(layer_ids, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    279\u001b[39m feat_ids = torch.cat(feat_ids, dim=\u001b[32m0\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m280\u001b[39m decoder_vectors = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    281\u001b[39m encoder_mapping = torch.cat(encoder_mapping, dim=\u001b[32m0\u001b[39m)\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pos_ids, layer_ids, feat_ids, decoder_vectors, encoder_mapping\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 15.62 GiB. GPU 0 has a total capacity of 44.34 GiB of which 10.35 GiB is free. Process 2491674 has 8.54 GiB memory in use. Process 2496736 has 25.44 GiB memory in use. Of the allocated memory 23.23 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import torch\n",
    "\n",
    "from circuit_tracer import ReplacementModel, attribute\n",
    "from circuit_tracer.utils import create_graph_files\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# FOr some reason this takes up a lot of VRAM\n",
    "graph = attribute(\n",
    "    prompt=prompt,\n",
    "    model=rm,\n",
    "    max_n_logits=max_n_logits,\n",
    "    desired_logit_prob=desired_logit_prob,\n",
    "    batch_size=batch_size,\n",
    "    max_feature_nodes=max_feature_nodes,\n",
    "    offload=offload,\n",
    "    verbose=verbose\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aafa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir = 'graphs'\n",
    "graph_name = 'example_graph.pt'\n",
    "graph_dir = Path(graph_dir)\n",
    "graph_dir.mkdir(exist_ok=True)\n",
    "graph_path = graph_dir / graph_name\n",
    "\n",
    "graph.to_pt(graph_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
