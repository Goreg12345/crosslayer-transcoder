{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Part 2.1: Introduction to Distributed Computing in PyTorch\n",
        "\n",
        "**PyTorch Translation of JAX Tutorial**\n",
        "\n",
        "**Author:** Translated from Phillip Lippe's JAX tutorial\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "Recent success in deep learning has been driven by the availability of large datasets and the ability to train large models on these datasets. However, training large models on large datasets is computationally expensive and usually goes beyond the capability of a single accelerator like a GPU. To speed up training, we can use parallelism to distribute the computation across multiple devices.\n",
        "\n",
        "This notebook introduces the basic concepts of distributed, multi-device processing in PyTorch. Unlike JAX's approach with explicit sharding and `shard_map`, PyTorch uses a different paradigm with process groups, collective operations, and abstractions like DistributedDataParallel (DDP).\n",
        "\n",
        "PyTorch's distributed computing is built around the concept of **processes** rather than **devices**. Each process typically manages one GPU and communicates with other processes through collective operations. This is different from JAX's device-centric approach where you explicitly shard arrays across devices.\n",
        "\n",
        "For this tutorial, we'll simulate multiple processes on a single machine to demonstrate the concepts. In practice, you would run separate Python processes, each managing one GPU.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Simulating 4 processes\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.multiprocessing as mp\n",
        "from torch.nn.parallel import DistributedDataParallel as DDP\n",
        "import numpy as np\n",
        "from typing import Any, Dict, Tuple, Optional\n",
        "import functools\n",
        "import warnings\n",
        "\n",
        "# For tutorial purposes, we'll use CPU and simulate multiple processes\n",
        "USE_CPU_ONLY = False\n",
        "WORLD_SIZE = 4  # Equivalent to 8 devices in JAX\n",
        "\n",
        "if USE_CPU_ONLY:\n",
        "    device = torch.device(\"cpu\")\n",
        "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "else:\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "print(f\"Using device: {device}\")\n",
        "print(f\"Simulating {WORLD_SIZE} processes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Distributed Computing in PyTorch\n",
        "\n",
        "This section introduces the basic concepts of distributed computing in PyTorch. Unlike JAX's explicit array sharding, PyTorch uses a process-based approach where each process typically owns one GPU and communicates through collective operations.\n",
        "\n",
        "### Key Differences from JAX:\n",
        "\n",
        "1. **Process-based vs Device-based**: PyTorch uses separate processes, each typically managing one GPU\n",
        "2. **Implicit vs Explicit sharding**: PyTorch often handles data distribution automatically (e.g., in DDP)\n",
        "3. **Collective operations**: Similar communication patterns but different API\n",
        "4. **Process groups**: Logical grouping of processes for communication\n",
        "\n",
        "### Basic Setup\n",
        "\n",
        "In PyTorch distributed computing, we need to:\n",
        "1. Initialize the process group\n",
        "2. Set up communication backend (NCCL for GPU, Gloo for CPU)\n",
        "3. Define rank (process ID) and world size (total processes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def setup_distributed(rank: int, world_size: int, backend: str = \"gloo\"):\n",
        "    \"\"\"Initialize distributed training setup.\n",
        "    \n",
        "    Args:\n",
        "        rank: Process rank (0 to world_size-1)\n",
        "        world_size: Total number of processes\n",
        "        backend: Communication backend ('nccl' for GPU, 'gloo' for CPU)\n",
        "    \"\"\"\n",
        "    os.environ['MASTER_ADDR'] = 'localhost'\n",
        "    os.environ['MASTER_PORT'] = '12355'\n",
        "    \n",
        "    # Initialize process group\n",
        "    dist.init_process_group(backend, rank=rank, world_size=world_size)\n",
        "    \n",
        "    print(f\"Process {rank}/{world_size} initialized\")\n",
        "    return rank, world_size\n",
        "\n",
        "def cleanup_distributed():\n",
        "    \"\"\"Clean up distributed training.\"\"\"\n",
        "    if dist.is_initialized():\n",
        "        dist.destroy_process_group()\n",
        "\n",
        "# For demonstration, we'll create utility functions that simulate distributed behavior\n",
        "class DistributedSimulator:\n",
        "    \"\"\"Simulates distributed operations for tutorial purposes.\"\"\"\n",
        "    \n",
        "    def __init__(self, world_size: int = 8):\n",
        "        self.world_size = world_size\n",
        "        self.rank = 0  # Current simulated rank\n",
        "    \n",
        "    def simulate_tensor_across_ranks(self, tensor: torch.Tensor) -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"Simulate how a tensor would be distributed across ranks.\"\"\"\n",
        "        if tensor.dim() == 1:\n",
        "            # Shard along first dimension\n",
        "            chunk_size = tensor.size(0) // self.world_size\n",
        "            chunks = {}\n",
        "            for rank in range(self.world_size):\n",
        "                start_idx = rank * chunk_size\n",
        "                end_idx = start_idx + chunk_size\n",
        "                chunks[rank] = tensor[start_idx:end_idx].clone()\n",
        "            return chunks\n",
        "        elif tensor.dim() == 2:\n",
        "            # Shard along first dimension (batch dimension)\n",
        "            chunk_size = tensor.size(0) // self.world_size\n",
        "            chunks = {}\n",
        "            for rank in range(self.world_size):\n",
        "                start_idx = rank * chunk_size\n",
        "                end_idx = start_idx + chunk_size\n",
        "                chunks[rank] = tensor[start_idx:end_idx].clone()\n",
        "            return chunks\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only 1D and 2D tensors supported in this demo\")\n",
        "    \n",
        "    def visualize_distribution(self, tensor_dict: Dict[int, torch.Tensor], name: str = \"tensor\"):\n",
        "        \"\"\"Visualize how tensor is distributed across ranks.\"\"\"\n",
        "        print(f\"\\n{name} distribution across {self.world_size} processes:\")\n",
        "        for rank, chunk in tensor_dict.items():\n",
        "            print(f\"  Rank {rank}: shape {chunk.shape}, data: {chunk.flatten()[:5].tolist()}{'...' if chunk.numel() > 5 else ''}\")\n",
        "\n",
        "# Create simulator instance\n",
        "sim = DistributedSimulator(WORLD_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Basic Tensor Distribution\n",
        "\n",
        "Let's start with a simple example of how tensors would be distributed across processes in PyTorch. Unlike JAX's explicit sharding, PyTorch typically handles this through data loaders and collective operations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original tensor: tensor([0., 1., 2., 3., 4., 5., 6., 7.])\n",
            "Device: cpu\n",
            "\n",
            "tensor 'a' distribution across 4 processes:\n",
            "  Rank 0: shape torch.Size([2]), data: [0.0, 1.0]\n",
            "  Rank 1: shape torch.Size([2]), data: [2.0, 3.0]\n",
            "  Rank 2: shape torch.Size([2]), data: [4.0, 5.0]\n",
            "  Rank 3: shape torch.Size([2]), data: [6.0, 7.0]\n",
            "\n",
            "tanh(a) distribution across 4 processes:\n",
            "  Rank 0: shape torch.Size([2]), data: [0.0, 0.7615941762924194]\n",
            "  Rank 1: shape torch.Size([2]), data: [0.9640275835990906, 0.9950547814369202]\n",
            "  Rank 2: shape torch.Size([2]), data: [0.9993293285369873, 0.9999092221260071]\n",
            "  Rank 3: shape torch.Size([2]), data: [0.9999877214431763, 0.9999983310699463]\n",
            "\n",
            "Gathered result: tensor([0.0000, 0.7616, 0.9640, 0.9951, 0.9993, 0.9999, 1.0000, 1.0000])\n",
            "Verification - matches torch.tanh(a): True\n"
          ]
        }
      ],
      "source": [
        "# Create a simple tensor (equivalent to JAX's jnp.arange(8))\n",
        "a = torch.arange(8, dtype=torch.float32)\n",
        "print(\"Original tensor:\", a)\n",
        "print(\"Device:\", a.device)\n",
        "\n",
        "# Simulate distribution across processes\n",
        "distributed_a = sim.simulate_tensor_across_ranks(a)\n",
        "sim.visualize_distribution(distributed_a, \"tensor 'a'\")\n",
        "\n",
        "# Apply operation to each chunk (simulating distributed computation)\n",
        "distributed_tanh = {}\n",
        "for rank, chunk in distributed_a.items():\n",
        "    distributed_tanh[rank] = torch.tanh(chunk)\n",
        "\n",
        "sim.visualize_distribution(distributed_tanh, \"tanh(a)\")\n",
        "\n",
        "# Gather results back (equivalent to collecting sharded results)\n",
        "gathered_result = torch.cat([distributed_tanh[rank] for rank in range(WORLD_SIZE)])\n",
        "print(f\"\\nGathered result: {gathered_result}\")\n",
        "print(f\"Verification - matches torch.tanh(a): {torch.allclose(gathered_result, torch.tanh(a))}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Collective Operations in PyTorch\n",
        "\n",
        "PyTorch provides collective operations similar to JAX's communication primitives. Here are the main equivalents:\n",
        "\n",
        "| JAX Operation | PyTorch Equivalent | Description |\n",
        "|---------------|-------------------|-------------|\n",
        "| `jax.lax.psum` | `dist.all_reduce` | Sum/average across processes |\n",
        "| `jax.lax.all_gather` | `dist.all_gather` | Gather tensors from all processes |\n",
        "| `jax.lax.psum_scatter` | `dist.reduce_scatter` | Reduce then scatter |\n",
        "| `jax.lax.ppermute` | `dist.send`/`dist.recv` | Point-to-point communication |\n",
        "\n",
        "Let's demonstrate these operations:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class CollectiveOperations:\n",
        "    \"\"\"Demonstrates PyTorch collective operations.\"\"\"\n",
        "    \n",
        "    def __init__(self, world_size: int):\n",
        "        self.world_size = world_size\n",
        "    \n",
        "    def simulate_all_reduce(self, tensors: Dict[int, torch.Tensor], op: str = \"sum\") -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"Simulate dist.all_reduce operation.\"\"\"\n",
        "        # Compute the reduction\n",
        "        if op == \"sum\":\n",
        "            reduced = sum(tensors.values())\n",
        "        elif op == \"mean\":\n",
        "            reduced = sum(tensors.values()) / len(tensors)\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported operation: {op}\")\n",
        "        \n",
        "        # All processes get the same result\n",
        "        return {rank: reduced.clone() for rank in range(self.world_size)}\n",
        "    \n",
        "    def simulate_all_gather(self, tensors: Dict[int, torch.Tensor]) -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"Simulate dist.all_gather operation.\"\"\"\n",
        "        # Gather all tensors\n",
        "        gathered = torch.cat([tensors[rank] for rank in range(self.world_size)])\n",
        "        \n",
        "        # All processes get the full gathered tensor\n",
        "        return {rank: gathered.clone() for rank in range(self.world_size)}\n",
        "    \n",
        "    def simulate_reduce_scatter(self, tensors: Dict[int, torch.Tensor]) -> Dict[int, torch.Tensor]:\n",
        "        \"\"\"Simulate dist.reduce_scatter operation.\"\"\"\n",
        "        # First reduce (sum all tensors)\n",
        "        reduced = sum(tensors.values())\n",
        "        \n",
        "        # Then scatter (each process gets a chunk)\n",
        "        chunk_size = reduced.size(0) // self.world_size\n",
        "        scattered = {}\n",
        "        for rank in range(self.world_size):\n",
        "            start_idx = rank * chunk_size\n",
        "            end_idx = start_idx + chunk_size\n",
        "            scattered[rank] = reduced[start_idx:end_idx].clone()\n",
        "        \n",
        "        return scattered\n",
        "\n",
        "# Create collective operations simulator\n",
        "collective_ops = CollectiveOperations(WORLD_SIZE)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### All-Reduce Operation (equivalent to JAX's psum)\n",
        "\n",
        "All-reduce is one of the most important operations in distributed training. It sums (or averages) tensors across all processes and gives each process the result.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create different values on each \"process\"\n",
        "values_per_rank = {rank: torch.tensor([float(rank + 1)]) for rank in range(WORLD_SIZE)}\n",
        "\n",
        "print(\"Before all_reduce:\")\n",
        "for rank, value in values_per_rank.items():\n",
        "    print(f\"  Rank {rank}: {value.item()}\")\n",
        "\n",
        "# Simulate all_reduce sum\n",
        "reduced_sum = collective_ops.simulate_all_reduce(values_per_rank, \"sum\")\n",
        "print(\"\\nAfter all_reduce (sum):\")\n",
        "for rank, value in reduced_sum.items():\n",
        "    print(f\"  Rank {rank}: {value.item()}\")\n",
        "\n",
        "# Simulate all_reduce mean\n",
        "reduced_mean = collective_ops.simulate_all_reduce(values_per_rank, \"mean\")\n",
        "print(\"\\nAfter all_reduce (mean):\")\n",
        "for rank, value in reduced_mean.items():\n",
        "    print(f\"  Rank {rank}: {value.item()}\")\n",
        "\n",
        "# Practical example: averaging gradients in distributed training\n",
        "print(\"\\n--- Practical Example: Gradient Averaging ---\")\n",
        "torch.manual_seed(42)\n",
        "gradients_per_rank = {rank: torch.randn(3) for rank in range(WORLD_SIZE)}\n",
        "\n",
        "print(\"Gradients on each rank:\")\n",
        "for rank, grad in gradients_per_rank.items():\n",
        "    print(f\"  Rank {rank}: {grad.numpy()}\")\n",
        "\n",
        "averaged_gradients = collective_ops.simulate_all_reduce(gradients_per_rank, \"mean\")\n",
        "print(\"\\nAveraged gradients (same on all ranks):\")\n",
        "print(f\"  All ranks: {averaged_gradients[0].numpy()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Practical Example: Distributed Data Parallel Training\n",
        "\n",
        "Let's create a practical example that shows how PyTorch's distributed training works in practice. This demonstrates the equivalent of JAX's shard_map approach but using PyTorch's process-based paradigm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SimpleModel(nn.Module):\n",
        "    \"\"\"Simple neural network for distributed training demonstration.\"\"\"\n",
        "    \n",
        "    def __init__(self, input_dim: int = 64, hidden_dim: int = 128, output_dim: int = 10):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.linear2(x)\n",
        "        return x\n",
        "\n",
        "def simulate_distributed_training_step(model, data_per_rank, targets_per_rank, world_size):\n",
        "    \"\"\"Simulate one step of distributed training.\"\"\"\n",
        "    \n",
        "    # Each rank computes loss and gradients on its data chunk\n",
        "    losses_per_rank = {}\n",
        "    gradients_per_rank = {}\n",
        "    \n",
        "    for rank in range(world_size):\n",
        "        # Forward pass on local data\n",
        "        local_data = data_per_rank[rank]\n",
        "        local_targets = targets_per_rank[rank]\n",
        "        \n",
        "        outputs = model(local_data)\n",
        "        loss = F.cross_entropy(outputs, local_targets)\n",
        "        losses_per_rank[rank] = loss\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        \n",
        "        # Collect gradients\n",
        "        local_gradients = []\n",
        "        for param in model.parameters():\n",
        "            if param.grad is not None:\n",
        "                local_gradients.append(param.grad.clone())\n",
        "        gradients_per_rank[rank] = local_gradients\n",
        "        \n",
        "        # Clear gradients for next rank simulation\n",
        "        model.zero_grad()\n",
        "    \n",
        "    return losses_per_rank, gradients_per_rank\n",
        "\n",
        "# Create model and data\n",
        "torch.manual_seed(0)\n",
        "model = SimpleModel(input_dim=64, hidden_dim=128, output_dim=10)\n",
        "batch_size = 32\n",
        "input_dim = 64\n",
        "\n",
        "# Create synthetic data distributed across ranks\n",
        "data = torch.randn(batch_size, input_dim)\n",
        "targets = torch.randint(0, 10, (batch_size,))\n",
        "\n",
        "# Distribute data across ranks\n",
        "data_per_rank = sim.simulate_tensor_across_ranks(data)\n",
        "targets_per_rank = sim.simulate_tensor_across_ranks(targets)\n",
        "\n",
        "print(\"Data distribution:\")\n",
        "sim.visualize_distribution(data_per_rank, \"training data\")\n",
        "\n",
        "# Simulate distributed training step\n",
        "losses, gradients = simulate_distributed_training_step(model, data_per_rank, targets_per_rank, WORLD_SIZE)\n",
        "\n",
        "print(f\"\\nLosses per rank:\")\n",
        "for rank, loss in losses.items():\n",
        "    print(f\"  Rank {rank}: {loss.item():.4f}\")\n",
        "\n",
        "# Simulate gradient averaging (all_reduce)\n",
        "print(f\"\\nGradient averaging across ranks:\")\n",
        "averaged_gradients = []\n",
        "for param_idx in range(len(gradients[0])):\n",
        "    param_grads = {rank: gradients[rank][param_idx] for rank in range(WORLD_SIZE)}\n",
        "    avg_grad = collective_ops.simulate_all_reduce(param_grads, \"mean\")\n",
        "    averaged_gradients.append(avg_grad[0])  # All ranks have same averaged gradient\n",
        "\n",
        "print(f\"Gradients averaged across {WORLD_SIZE} ranks\")\n",
        "print(f\"Number of parameter groups: {len(averaged_gradients)}\")\n",
        "for i, grad in enumerate(averaged_gradients[:2]):  # Show first 2 parameter gradients\n",
        "    print(f\"  Param {i} gradient shape: {grad.shape}, norm: {grad.norm().item():.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Real PyTorch Distributed Training\n",
        "\n",
        "In practice, PyTorch distributed training is much simpler than our simulation. Here's how you would actually implement distributed training:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def real_distributed_training_example():\n",
        "    \"\"\"Shows how real PyTorch distributed training would work.\"\"\"\n",
        "    \n",
        "    # This is the code you would actually write for distributed training\n",
        "    code_example = '''\n",
        "def train_distributed(rank, world_size):\n",
        "    # 1. Setup\n",
        "    setup_distributed(rank, world_size)\n",
        "    \n",
        "    # 2. Create model and wrap with DDP\n",
        "    model = SimpleModel()\n",
        "    if torch.cuda.is_available():\n",
        "        model = model.cuda(rank)\n",
        "    model = DDP(model, device_ids=[rank] if torch.cuda.is_available() else None)\n",
        "    \n",
        "    # 3. Create distributed sampler for data\n",
        "    from torch.utils.data import DataLoader, DistributedSampler\n",
        "    dataset = YourDataset()\n",
        "    sampler = DistributedSampler(dataset, num_replicas=world_size, rank=rank)\n",
        "    dataloader = DataLoader(dataset, batch_size=32, sampler=sampler)\n",
        "    \n",
        "    # 4. Training loop\n",
        "    optimizer = torch.optim.Adam(model.parameters())\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        sampler.set_epoch(epoch)  # Important for proper shuffling\n",
        "        \n",
        "        for batch_idx, (data, targets) in enumerate(dataloader):\n",
        "            if torch.cuda.is_available():\n",
        "                data, targets = data.cuda(rank), targets.cuda(rank)\n",
        "            \n",
        "            # Forward pass\n",
        "            outputs = model(data)\n",
        "            loss = F.cross_entropy(outputs, targets)\n",
        "            \n",
        "            # Backward pass - DDP automatically handles gradient averaging\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "    \n",
        "    cleanup_distributed()\n",
        "\n",
        "# Launch training with:\n",
        "# mp.spawn(train_distributed, args=(world_size,), nprocs=world_size, join=True)\n",
        "'''\n",
        "    \n",
        "    print(\"Real PyTorch Distributed Training Code:\")\n",
        "    print(code_example)\n",
        "    \n",
        "    print(\"\\nKey differences from our simulation:\")\n",
        "    print(\"1. DistributedDataParallel (DDP) automatically handles gradient averaging\")\n",
        "    print(\"2. DistributedSampler ensures each process gets different data\")\n",
        "    print(\"3. You launch multiple processes with mp.spawn() or torchrun\")\n",
        "    print(\"4. Each process runs the same code but on different data\")\n",
        "\n",
        "# Show the example\n",
        "real_distributed_training_example()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Summary: JAX vs PyTorch Distributed Computing\n",
        "\n",
        "Here's a comprehensive comparison of distributed computing concepts between JAX and PyTorch:\n",
        "\n",
        "### Architecture Differences\n",
        "\n",
        "| Aspect | JAX | PyTorch |\n",
        "|--------|-----|---------|\n",
        "| **Paradigm** | Device-centric sharding | Process-centric communication |\n",
        "| **Explicit Control** | High (explicit sharding) | Medium (abstracted by DDP) |\n",
        "| **Setup** | Mesh + PartitionSpec | Process groups + backends |\n",
        "| **SPMD** | `shard_map` functions | Multiple processes with same code |\n",
        "\n",
        "### Communication Operations\n",
        "\n",
        "| Operation | JAX | PyTorch | Notes |\n",
        "|-----------|-----|---------|-------|\n",
        "| **Sum/Average** | `jax.lax.psum` | `dist.all_reduce` | Both sum across devices/processes |\n",
        "| **Gather** | `jax.lax.all_gather` | `dist.all_gather` | Collect data from all devices/processes |\n",
        "| **Scatter-Sum** | `jax.lax.psum_scatter` | `dist.reduce_scatter` | Reduce then distribute chunks |\n",
        "| **Point-to-point** | `jax.lax.ppermute` | `dist.send`/`dist.recv` | Direct device-to-device communication |\n",
        "\n",
        "### Practical Usage\n",
        "\n",
        "**JAX Approach:**\n",
        "- Explicit control over device placement\n",
        "- Write per-device code with `shard_map`\n",
        "- Manual handling of communication\n",
        "- Great for research and custom parallelism strategies\n",
        "\n",
        "**PyTorch Approach:**\n",
        "- Higher-level abstractions (DDP, FSDP)\n",
        "- Write single-process code, run multiple processes\n",
        "- Automatic gradient synchronization\n",
        "- Better for standard training workflows\n",
        "\n",
        "### When to Choose What?\n",
        "\n",
        "**Choose JAX if:**\n",
        "- You need fine-grained control over device placement\n",
        "- You're implementing custom parallelism strategies\n",
        "- You prefer functional programming paradigms\n",
        "- You're doing research requiring explicit control\n",
        "\n",
        "**Choose PyTorch if:**\n",
        "- You want easier setup for standard distributed training\n",
        "- You prefer object-oriented programming\n",
        "- You need extensive ecosystem support\n",
        "- You're building production systems\n",
        "\n",
        "Both frameworks are powerful for distributed computing, but they approach the problem differently. JAX gives you more explicit control, while PyTorch provides higher-level abstractions that are easier to use for common scenarios.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
